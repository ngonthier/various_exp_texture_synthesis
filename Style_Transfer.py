#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Apr 14 15:32:49 2017

The goal of this script is to code the Style Transfer Algorithm 

Inspired from https://github.com/cysmith/neural-style-tf/blob/master/neural_style.py
and https://github.com/leongatys/PytorchNeuralStyleTransfer/blob/master/NeuralStyleTransfer.ipynb

@author: nicolas
"""
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='0' # 1 to remove info, 2 to remove warning and 3 for all
import tensorflow as tf
import scipy.io
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import time
import pickle
import math
from tensorflow.python.client import timeline
from Arg_Parser import get_parser_args 
import utils
from numpy.fft import fft2, ifft2
from skimage.color import gray2rgb
import Misc
import cv2
from shutil import copyfile
from functools import partial

# Name of the 19 first layers of the VGG19
VGG19_LAYERS = (
    'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',

    'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',

    'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',
    'relu3_3', 'conv3_4', 'relu3_4', 'pool3',

    'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',
    'relu4_3', 'conv4_4', 'relu4_4', 'pool4',

    'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',
    'relu5_3', 'conv5_4', 'relu5_4')
#layers   = [2 5 10 19 28]; for texture generation
style_layers_size =  {'input':3,'conv1' : 64,'relu1' : 64,'pool1': 64,'conv2' : 128,'relu2' : 128,'pool2':128,'conv3' : 256,'relu3' : 256,'pool3':256,'conv4': 512,'relu4' : 512,'pool4':512,'conv5' : 512,'relu5' : 512,'pool5':512}
# TODO : check if the N value are right for the poolx

def test_version_sup(version_str):
    version_str_tab = version_str.split('.')
    tf_version_teb =  tf.__version__.split('.')
    status = False
    for a,b in zip(tf_version_teb,version_str_tab):
        if float(a) > float(b):
            status = True
    return(status)

def plot_image(path_to_image):
    """
    Function to plot an image
    """
    img = Image.open(path_to_image)
    plt.imshow(img)
    
def get_vgg_layers(VGG19_mat='normalizedvgg.mat'):
    """
    Load the VGG 19 layers
    """
    if(VGG19_mat=='imagenet-vgg-verydeep-19.mat') or (VGG19_mat=='random_net.mat'):
        # The vgg19 network from http://www.vlfeat.org/matconvnet/pretrained/
        try:
            vgg_rawnet = scipy.io.loadmat(VGG19_mat)
            vgg_layers = vgg_rawnet['layers'][0]
        except(FileNotFoundError):
            print("The path to the VGG19_mat is not right or the .mat is not here")
            print("You can download it here : http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat")
            raise
    elif(VGG19_mat=='normalizedvgg.mat') or (VGG19_mat=='zero_net.mat'): # Normalized VGG19 network over images (see Gatys Texture papers) generated by Gang with the Caffe model provide by Gatys
        try:
            vgg_rawnet = scipy.io.loadmat(VGG19_mat)
            vgg_layers = vgg_rawnet['net'][0]['layers'][0][0]
        except(FileNotFoundError):
            print("The path to the VGG19_mat is not right or the .mat is not here")
            print("You have to  get the weight from https://github.com/leongatys/DeepTextures and convert them to .mat format.")
            raise
    else:
        print("The path to the VGG19_mat is unknown.")
    return(vgg_layers)

def net_preloaded(vgg_layers, input_image,pooling_type='avg',padding='SAME'):
    """
    This function read the vgg layers and create the net architecture
    We need the input image to know the dimension of the input layer of the net
    """
    
    net = {}
    _,height, width, numberChannels = input_image.shape # In order to have the right shape of the input
    current = tf.Variable(np.zeros((1, height, width, numberChannels), dtype=np.float32))
    net['input'] = current
    for i, name in enumerate(VGG19_LAYERS):
        kind = name[:4]
        if(kind == 'conv'):
            # Only way to get the weight of the kernel of convolution
            # Inspired by http://programtalk.com/vs2/python/2964/facenet/tmp/vggverydeep19.py/
            kernels = vgg_layers[i][0][0][2][0][0] 
            bias = vgg_layers[i][0][0][2][0][1]
            # matconvnet: weights are [width, height, in_channels, out_channels]
            # tensorflow: weights are [height, width, in_channels, out_channels]
            #kernels = tf.constant(np.transpose(kernels, (1,0 ,2, 3)))
            kernels = tf.constant(kernels)
            bias = tf.constant(bias.reshape(-1))
            current = conv_layer(current, kernels, bias,name,padding) 
            # Update the  variable named current to have the right size
        elif(kind == 'relu'):
            current = tf.nn.relu(current,name=name)
        elif(kind == 'pool'):
            current = pool_layer(current,name,pooling_type,padding)

        net[name] = current

    assert len(net) == len(VGG19_LAYERS) +1 # Test if the length is right 
    return(net)

def conv_layer(input, weights, bias,name,padding='SAME'):
    """
    This function create a conv2d with the already known weight and bias
    
    conv2d :
    Computes a 2-D convolution given 4-D input and filter tensors.
    input: A Tensor. Must be one of the following types: half, float32, float64
    Given an input tensor of shape [batch, in_height, in_width, in_channels] and 
    a filter / kernel tensor of shape 
    [filter_height, filter_width, in_channels, out_channels]
    """
    stride = 1
    if(padding=='SAME' or padding=='VALID'):
        conv = tf.nn.conv2d(input, weights, strides=(1, stride, stride, 1),
            padding=padding,name=name)
    elif(padding=='Circular'):
        input = get_img_2pixels_more(input)
        conv = tf.nn.conv2d(input, weights, strides=(1, stride, stride, 1),
            padding='VALID',name=name)
    elif(padding=='Davy'): # Real padding we will drop some pixel on the image
        conv = tf.nn.conv2d(input, weights, strides=(1, stride, stride, 1),
            padding='VALID',name=name)
    # We need to impose the weights as constant in order to avoid their modification
    # when we will perform the optimization
    return(tf.nn.bias_add(conv, bias))

def get_img_2pixels_more(input):
    new_input = tf.concat([input,input[:,0:2,:,:]],axis=1)
    new_input = tf.concat([new_input,new_input[:,:,0:2,:]],axis=2)
    return(new_input)

def pool_layer(input,name,pooling_type='avg',padding='SAME'):
    """
    Average pooling on windows 2*2 with stride of 2
    input is a 4D Tensor of shape [batch, height, width, channels]
    Each pooling op uses rectangular windows of size ksize separated by offset 
    strides in the avg_pool function 
    """
    stride_pool = 2
    if(padding== 'Circular'): # TODO Test if paire ou impaire !!! 
        _,h,w,_ = input.shape
        if not(h%2==0):
            input = tf.concat([input,input[:,0:2,:,:]],axis=1)
        if not(w%2==0):
            input = tf.concat([input,input[:,:,0:2,:]],axis=2)
        padding = 'VALID'
    if(padding=='Davy'):
        padding = 'VALID'
    if pooling_type == 'avg':
        pool = tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, stride_pool, stride_pool, 1),
                padding=padding,name=name) 
    elif pooling_type == 'max':
        pool = tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, stride_pool, stride_pool, 1),
                padding=padding,name=name) 
    return(pool)

def loss_LowResConstr(sess,net,scale,i_lowres,weightMultiplicatif=1.):
    """
    Compute the content term of the loss function on the input X and a lower version of it
    """
    weight_help_convergence = 10**9 
    x = net['input']
    #x_lowres = tf.image.resize_area(x,(scale,scale),align_corners=False)
    x_lowres = tf.image.resize_images(x,(scale[0],scale[1]),method=tf.image.ResizeMethod.BILINEAR,align_corners=False)
    M = scale[0]*scale[1]
    loss = tf.nn.l2_loss(tf.subtract(x_lowres,i_lowres))*(weightMultiplicatif*weight_help_convergence/tf.to_float(M**2))    
    return(loss)

def sum_content_losses(sess, net, dict_features_repr,M_dict,content_layers):
    """
    Compute the content term of the loss function
    Input : 
    - the tensforflow session sess
    - the vgg19 net
    - the dictionnary of the content image representation thanks to the net
    """
    length_content_layers = float(len(content_layers))
    weight_help_convergence = 10**9 # Need to multiply by 10**9 to have a big enough gradient for the LBFGS algo
    content_loss = 0
    for layer, weight in content_layers:
        M = M_dict[layer]
        P = tf.constant(dict_features_repr[layer])
        F = net[layer]
        content_loss +=  tf.nn.l2_loss(tf.subtract(P,F))*(
            weight*weight_help_convergence/(length_content_layers*(tf.to_float(M)**2)))
    return(content_loss)

def sum_style_losses(sess, net, dict_gram,M_dict,style_layers):
    """
    Compute the style term of the loss function with Gram Matrix from the
    Gatys Paper
    Input : 
    - the tensforflow session sess
    - the vgg19 net
    - the dictionnary of Gram Matrices
    - the dictionnary of the size of the image content through the net
    """
    # Info for the vgg19
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**(9) # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    for layer, weight in style_layers:
        # For one layer
        N = style_layers_size[layer[:5]]
        A = dict_gram[layer]
        A = tf.constant(A)
        # Get the value of this layer with the generated image
        M = M_dict[layer]
        x = net[layer]
        G = gram_matrix(x,N,M) # Nota Bene : the Gram matrix is normalized by M
        style_loss = tf.nn.l2_loss(tf.subtract(G,A))  # output = sum(t ** 2) / 2
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    return(total_style_loss)

def style_losses_audetail(sess, net, dict_gram,M_dict,style_layers):
    """
    Compute the style term of the loss function with Gram Matrix from the
    Gatys Paper
    Input : 
    - the tensforflow session sess
    - the vgg19 net
    - the dictionnary of Gram Matrices
    - the dictionnary of the size of the image content through the net
    Return an array of the style losses for the differents elements of the layers !!! 
    """
    # Info for the vgg19
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**(9) # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    style_losses_tab = []
    for i, couple in enumerate(style_layers):
        layer, weight = couple
        # For one layer
        N = style_layers_size[layer[:5]]
        A = dict_gram[layer]
        A = tf.constant(A)
        # Get the value of this layer with the generated image
        M = M_dict[layer]
        x = net[layer]
        G = gram_matrix(x,N,M) # Nota Bene : the Gram matrix is normalized by M
        #print(tf.shape(G))
        gram_diff = tf.multiply(tf.pow(tf.subtract(G,A),2),weight * weight_help_convergence  / (4.*(N**2)*length_style_layers))
        #print(tf.shape(gram_diff))
        style_losses_tab += [gram_diff]
        total_style_loss += tf.reduce_sum(gram_diff)
    return(total_style_loss,style_losses_tab)

def style_losses(sess, net, dict_gram,M_dict,style_layers):
    """
    Compute the style term of the loss function with Gram Matrix from the
    Gatys Paper
    Input : 
    - the tensforflow session sess
    - the vgg19 net
    - the dictionnary of Gram Matrices
    - the dictionnary of the size of the image content through the net
    Return an array of the style losses for the differents layers and the total_style_loss
    """
    # Info for the vgg19
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**(9) # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    style_losses_tab = []
    for i, couple in enumerate(style_layers):
        layer, weight = couple
        # For one layer
        N = style_layers_size[layer[:5]]
        A = dict_gram[layer]
        A = tf.constant(A)
        # Get the value of this layer with the generated image
        M = M_dict[layer]
        x = net[layer]
        G = gram_matrix(x,N,M) # Nota Bene : the Gram matrix is normalized by M
        style_loss = tf.nn.l2_loss(tf.subtract(G,A))  # output = sum(t ** 2) / 2
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        style_losses_tab += [style_loss]
        total_style_loss += style_loss
    return(total_style_loss,style_losses_tab)

def texture_loss_wt_mask(sess, net, dict_gram,M_dict,mask_dict,style_layers):
    """
    Multiply the first layer 
    """
    # Info for the vgg19
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**(9) # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    for layer, weight in style_layers:
        # For one layer
        N = style_layers_size[layer[:5]]
        NN = N**2
        A = dict_gram[layer]
        A = tf.constant(A)
        # Get the value of this layer with the generated image
        M = M_dict[layer]
        x = net[layer]
        G = gram_matrix(x,N,M) # Nota Bene : the Gram matrix is normalized by M
        diff = tf.subtract(G,A)
        mask = mask_dict[layer]
        diff = tf.multiply(diff,mask)
        NN = np.sum(mask)
        print("Number of non null element in mask ",NN)
        style_loss = tf.nn.l2_loss(diff)  # output = sum(t ** 2) / 2
        style_loss *=  weight * weight_help_convergence  / (2.*(NN)*length_style_layers)
        total_style_loss += style_loss
    return(total_style_loss)


def compute_4_moments(x):
    """
    Compute the 4 first moments of the features (response of the kernel) 
    of a 4D Tensor
    """
    # TODO : this is biased moment !! 
    mean_x = tf.reduce_mean(x, axis=[0,1,2])
    variance_x = tf.subtract(tf.reduce_mean(tf.pow(x,2), axis=[0,1,2]),mean_x)
    sig_x = tf.sqrt(variance_x)
    skewness_x = tf.reduce_mean(tf.pow(tf.divide(tf.subtract(x,mean_x),sig_x),3), axis=[0,1,2])
    kurtosis_x = tf.reduce_mean(tf.pow(tf.divide(tf.subtract(x,mean_x),sig_x),4), axis=[0,1,2])
    return(mean_x,variance_x,skewness_x,kurtosis_x)
        
    
def compute_n_moments_reduce(x,n,axis=[0,1,2]):
    """
    Compute the n first moments of the features (response of the kernel)
    The moments are reduce and centered but beacareful for some texture and layer there are a risk of getting a nan 
    as result due to ssig_x that goes to 0 otherwise used compute_n_moments
    """
    assert(n > 0)
    mean_x = tf.reduce_mean(x,axis=axis)
    list_of_moments = [mean_x]
    if(n>1):
        variance_x =   tf.reduce_mean(tf.pow(tf.subtract(x,mean_x),2),axis=axis)
        list_of_moments += [variance_x]
        sig_x =  tf.sqrt(variance_x)
    if(n>2):
        for r in range(3,n+1,1):
            moment_r = tf.reduce_mean(tf.pow(tf.divide(tf.subtract(x,mean_x),sig_x),r), axis=axis) # Centré/réduit
            tf.where(tf.is_nan(moment_r), tf.zeros_like(moment_r), moment_r  )
            list_of_moments += [moment_r]
    return(list_of_moments)
    
def compute_n_moments(x,n,axis=[0,1,2]):
    """
    Compute the n first moments of the features (response of the kernel)
    Only the centered moments and not the centered reduced ones ! otherwise risk to have a nan number because of sig_x
    """
    assert(n > 0)
    mean_x = tf.reduce_mean(x,axis=axis)
    list_of_moments = [mean_x]
    if(n>1):
        variance_x =   tf.reduce_mean(tf.pow(tf.subtract(x,mean_x),2),axis=axis)
        list_of_moments += [variance_x]
    if(n>2):
        for r in range(3,n+1,1):
            #sig_x =  tf.sqrt(variance_x)
            sig_x = 1.
            moment_r = tf.reduce_mean(tf.pow(tf.divide(tf.subtract(x,mean_x),sig_x),r), axis=axis) # Centré
            tf.where(tf.is_nan(moment_r), tf.zeros_like(moment_r), moment_r  )
            # TODO : change that to some thing more optimal : pb computation of the power several times
            list_of_moments += [moment_r]
    return(list_of_moments)
    
def compute_Lp_norm(x,p):
    """
    Compute the p first Lp norm of the features
    """
    assert(p > 0)
    list_of_Lp = []
    for r in range(1,p+1,1):
        L_r_x = tf.pow(tf.reduce_mean(tf.pow(tf.abs(x),r), axis=[0,1,2]),1./r) 
        #F_x = tf.reshape(x,[M_i_1,N_i_1])
        #L_r_x =tf.norm(x,ord=r,axis=[0,1],name=str(r)) 
        # TODO : change that to some thing more optimal : pb computation of the power several times
        list_of_Lp += [L_r_x]
    return(list_of_Lp)
        
def sum_style_stats_loss(sess,net,image_style,M_dict,style_layers):
    """
    Compute a loss that is the l2 norm of the 4th moment of the optimization
    """
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**9 # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    sess.run(net['input'].assign(image_style))
    for layer, weight in style_layers:
        # For one layer
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer] # response to the layer 
        mean_x,variance_x,skewness_x,kurtosis_x = compute_4_moments(x)
        mean_a,variance_a,skewness_a,kurtosis_a = compute_4_moments(a)
        style_loss = tf.nn.l2_loss(tf.subtract(mean_x,mean_a)) + tf.nn.l2_loss(tf.subtract(variance_x,variance_a)) + tf.nn.l2_loss(tf.subtract(skewness_x,skewness_a)) + tf.nn.l2_loss(tf.subtract(kurtosis_x,kurtosis_a))
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    return(total_style_loss)

def loss_n_moments(sess,net,image_style,M_dict,n,style_layers):
    """
    Compute a loss that is the l2 norm of the nth moment of the optimization
    """
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**9 # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    sess.run(net['input'].assign(image_style))
    for layer, weight in style_layers:
        # For one layer
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer] # response to the layer 
        moments_x = compute_n_moments(x,n)
        moments_a = compute_n_moments(a,n)
        style_loss = sum(map(tf.nn.l2_loss,map(tf.subtract, moments_x,moments_a)))
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers) # Normalized by the number of features N
        total_style_loss += style_loss
    return(total_style_loss)

def loss_n_stats(sess,net,image_style,M_dict,n,style_layers,TypeOfComputation='moments'):
    """
    Compute a loss that is the l2 norm of the n element of a statistic on 
    the features maps : moments or norm
    """
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**(9)
    # Because the function is pretty flat 
    total_style_loss = 0
    sess.run(net['input'].assign(image_style))
    for layer, weight in style_layers:
        # For one layer
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer] # response to the layer 
        if(TypeOfComputation=='moments'):
            stats_x = compute_n_moments(x,n)
            stats_a = compute_n_moments(a,n)
        elif(TypeOfComputation=='Lp'):
            stats_x = compute_Lp_norm(x,n)
            stats_a = compute_Lp_norm(a,n)
        elif(TypeOfComputation=='nmoments_reduce'):
            stats_x = compute_n_moments_reduce(x,n)
            stats_a = compute_n_moments_reduce(a,n)
        style_loss = sum(map(tf.nn.l2_loss,map(tf.subtract, stats_x,stats_a)))
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers) # Normalized by the number of features N
        total_style_loss += style_loss
    return(total_style_loss)
    
def loss_variance(sess,net,image_style,M_dict,style_layers):
    """
    Compute a loss that is the l2 norm of the variance between the two 
    image (reference and the one we are optimizing)
    """
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**(9)
    # Because the function is pretty flat 
    total_style_loss = 0
    sess.run(net['input'].assign(image_style))
    for layer, weight in style_layers:
        # For one layer
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer] # response to the layer 
        axis = [0,1,2]
        mean_x,variance_x = tf.nn.moments(x,axes=axis)
        mean_a,variance_a = tf.nn.moments(tf.convert_to_tensor(a, dtype=tf.float32),axes=axis)
        style_loss = tf.nn.l2_loss(tf.subtract(variance_x,variance_a))
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers) # Normalized by the number of features N
        total_style_loss += style_loss
    return(total_style_loss)
    

def loss_p_norm(sess,net,image_style,M_dict,p,style_layers): # Faire une fonction génértique qui prend en entree le type de norme !!! 
    length_style_layers = float(len(style_layers))
    weight_help_convergence = 10**9 # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    sess.run(net['input'].assign(image_style))
    for layer, weight in style_layers:
        # For one layer
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer] # response to the layer 
        L_p_x = compute_Lp_norm(x,p) # Les Lp sont positives, on cherche juste à egaliser les énergies la
        L_p_a = compute_Lp_norm(a,p)
        style_loss = sum(map(tf.nn.l2_loss,map(tf.subtract, L_p_x,L_p_a)))
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers) # Normalized by the number of features N
        total_style_loss += style_loss
    return(total_style_loss)
    

def loss_crosscor_inter_scale(sess,net,image_style,M_dict,style_layers,sampling='down',pooling_type='avg'):
    """
    Compute a loss that is the l2 norm of the cross correlation of the previous band
    The sampling argument is down for downsampling and up for up sampling
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9 # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0.
    sess.run(net['input'].assign(image_style))
    if(length_style_layers_int > 1):
        for i in range(length_style_layers_int-1):
            layer_i, weight_i = style_layers[i]
            layer_i_1, weight_i_1 = style_layers[i+1]
            N_i = style_layers_size[layer_i[:5]]
            N_i_1 = style_layers_size[layer_i_1[:5]]
            M_i_1 = M_dict[layer_i_1[:5]]
            #print("M_i,M_i_1,N_i",M_i,M_i_1,N_i)
            x_i = net[layer_i]
            x_i_1 = net[layer_i_1]
            a_i = sess.run(net[layer_i])
            a_i_1 = sess.run(net[layer_i_1]) # TODO change this is suboptimal because youcompute twice a_i !! 
            if(sampling=='down'):
                if(pooling_type=='avg'):
                    x_i = tf.nn.avg_pool(x_i, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),padding='SAME')
                    a_i = tf.nn.avg_pool(a_i, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),padding='SAME')
                elif(pooling_type == 'max'):
                    x_i = tf.nn.max_pool(x_i, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),padding='SAME') 
                    a_i = tf.nn.max_pool(a_i, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),padding='SAME') 
                _,height,width,_ = x_i.shape
                M_i = tf.to_int32(height*width)
            elif(sampling=='up'):
                _,new_height,new_width,_ = x_i.shape
                _,height,_,_ = x_i_1.shape
                if(layer_i[:5]==layer_i_1[:5]):
                    factor = 1 # Not upsample
                else:
                    factor = 2
                upsample_filter_np = utils.bilinear_upsample_weights(factor,N_i_1)
                x_i_1 = tf.nn.conv2d_transpose(x_i_1, upsample_filter_np,
                        output_shape=[1, tf.to_int32(new_height), tf.to_int32(new_width), N_i_1],
                        strides=[1, factor, factor, 1])
                a_i_1 = tf.nn.conv2d_transpose(a_i_1, upsample_filter_np,
                        output_shape=[1, tf.to_int32(new_height), tf.to_int32(new_width), N_i_1],
                        strides=[1, factor, factor, 1])
                M_i = tf.to_int32(new_height*new_width)
                M_i_1 = M_i
            F_x_i = tf.reshape(x_i,[M_i,N_i])
            F_x_i_1 = tf.reshape(x_i_1,[M_i_1,N_i_1])
            G_x = tf.matmul(tf.transpose(F_x_i),F_x_i_1)
            G_x /= tf.to_float(M_i)
            F_a_i = tf.reshape(a_i,[M_i,N_i])
            F_a_i_1 = tf.reshape(a_i_1,[M_i_1,N_i_1])
            G_a = tf.matmul(tf.transpose(F_a_i),F_a_i_1)
            G_a /= tf.to_float(M_i)
            style_loss = tf.nn.l2_loss(tf.subtract(G_x,G_a))  # output = sum(t ** 2) / 2
            # TODO selon le type de style voulu soit reshape the style image sinon Mcontenu/Mstyle
            weight= (weight_i + weight_i_1) /2.
            style_loss *=  weight * weight_help_convergence  / (2.*(N_i*N_i_1)*length_style_layers)
            total_style_loss += style_loss
    return(total_style_loss)

def pgcd(a,b) :  
    while a%b != 0 : 
        a, b = b, a%b 
    return b

def loss_autocorrbizarre(sess,net,image_style,M_dict,style_layers):
    """
    Computation of the autocorrelation of the filters
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9
    total_style_loss = 0.

    sess.run(net['input'].assign(image_style))  
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        F_x = tf.fft2d(tf.complex(x,0.))
        R_x = tf.real(tf.multiply(F_x,tf.conj(F_x)))
        R_x /= tf.to_float(M**2)
        F_a = tf.fft2d(tf.complex(a,0.))
        R_a = tf.real(tf.multiply(F_a,tf.conj(F_a)))
        R_a /= tf.to_float(M**2)
        style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)

def loss_autocorr(sess,net,image_style,M_dict,style_layers,gamma_autocorr=1.):
    """
    Computation of the autocorrelation of the filters
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = (10**9)
    total_style_loss = 0.
    
    _, h_a, w_a, N = image_style.shape      
    sess.run(net['input'].assign(image_style))
        
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        x = tf.transpose(x, [0,3,1,2])
        a = tf.transpose(a, [0,3,1,2])
        F_x = tf.fft2d(tf.complex(x,0.))
        R_x = tf.real(tf.multiply(F_x,tf.conj(F_x))) # Module de la transformee de Fourrier : produit terme a terme
        R_x /= tf.to_float(M**2) # Normalisation du module de la TF
        F_a = tf.fft2d(tf.complex(a,0.))
        R_a = tf.real(tf.multiply(F_a,tf.conj(F_a))) # Module de la transformee de Fourrier
        R_a /= tf.to_float(M**2)
        style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        style_loss *=  gamma_autocorr* weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def loss_autocorr_directSpace(sess,net,image_style,M_dict,style_layers,gamma_autocorr=1.):
    """
    Computation of the autocorrelation of the filters
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    return(0)
    #length_style_layers_int = len(style_layers)
    #length_style_layers = float(length_style_layers_int)
    #weight_help_convergence = (10**9)
    #total_style_loss = 0.
    
    #_, h_a, w_a, N = image_style.shape      
    #sess.run(net['input'].assign(image_style))
        
    #for layer, weight in style_layers:
        #N = style_layers_size[layer[:5]]
        #M = M_dict[layer]
        #a = sess.run(net[layer])
        #x = net[layer]
        #x = tf.transpose(x, [0,3,1,2])
        #a = tf.transpose(a, [0,3,1,2])
        #F_x = tf.fft2d(tf.complex(x,0.))
        #R_x = tf.real(tf.multiply(F_x,tf.conj(F_x))) # Module de la transformee de Fourrier : produit terme a terme
        #R_x /= tf.to_float(M**2) # Normalisation du module de la TF
        #F_a = tf.fft2d(tf.complex(a,0.))
        #R_a = tf.real(tf.multiply(F_a,tf.conj(F_a))) # Module de la transformee de Fourrier
        #R_a /= tf.to_float(M**2)
        #style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        #style_loss *=  gamma_autocorr* weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        #total_style_loss += style_loss
    #total_style_loss =tf.to_float(total_style_loss)
    #return(total_style_loss)
    
def loss_autocorrLog(sess,net,image_style,M_dict,style_layers):
    """
    Computation of the autocorrelation of the filters normalise en passant
    par le log
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = (10**9)
    total_style_loss = 0.
    
    _, h_a, w_a, N = image_style.shape      
    sess.run(net['input'].assign(image_style))
        
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        x = tf.transpose(x, [0,3,1,2])
        a = tf.transpose(a, [0,3,1,2])
        F_x = tf.fft2d(tf.complex(x,0.))
        R_x = tf.pow(tf.real(tf.multiply(F_x,tf.conj(F_x))),0.5) # Module de la transformee de Fourrier : produit terme a terme
        R_x /= tf.to_float(M) # Normalisation du module de la TF
        R_x = tf.log(1+R_x)
        F_a = tf.fft2d(tf.complex(a,0.))
        R_a = tf.pow(tf.real(tf.multiply(F_a,tf.conj(F_a))),0.5) # Module de la transformee de Fourrier
        R_a /= tf.to_float(M)
        R_a = tf.log(1+R_a)
        style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def loss_autocorr_rfft(sess,net,image_style,M_dict,style_layers):
    """
    Computation of the autocorrelation of the filters
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = (10**9)
    total_style_loss = 0.
    
    _, h_a, w_a, N = image_style.shape      
    sess.run(net['input'].assign(image_style))
        
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        x = tf.transpose(x, [0,3,1,2])
        a = tf.transpose(a, [0,3,1,2])
        F_x = tf.spectral.rfft2d(x)
        F_a = tf.spectral.rfft2d(a)
        
        #R_x = tf.abs(F_x) # Module de la transformee de Fourrier : produit terme a terme
        #R_x /= tf.to_float(M) # Normalisation du module de la TF
        #R_a = tf.abs(F_a) # Module de la transformee de Fourrier
        #R_a /= tf.to_float(M)
        
        R_x = tf.real(tf.multiply(F_x,tf.conj(F_x))) # Module de la transformee de Fourrier : produit terme a terme
        R_x /= tf.to_float(M**2) # Normalisation du module de la TF
        R_a = tf.real(tf.multiply(F_a,tf.conj(F_a))) # Module de la transformee de Fourrier
        R_a /= tf.to_float(M**2)
        
        style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def loss_fft_vect(sess,net,image_style,M_dict,style_layers):
    """
    Computation of the autocorrelation of the filters consider as vector
    kind of vectorized FFT
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = (10**9)
    total_style_loss = 0.
    
    _, h_a, w_a, N = image_style.shape      
    sess.run(net['input'].assign(image_style))
        
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        x = tf.transpose(x, [0,3,1,2])
        a = tf.transpose(a, [0,3,1,2])
        F_x = tf.fft2d(tf.complex(x,0.))
        R_x = tf.real(tf.multiply(F_x,tf.conj(F_x))) # Module de la transformee de Fourrier : produit terme a terme
        R_x /= tf.to_float(M**2) # Normalisation du module de la TF
        R_x = tf.reduce_mean(R_x,axis=1)
        F_a = tf.fft2d(tf.complex(a,0.))
        R_a = tf.real(tf.multiply(F_a,tf.conj(F_a))) # Module de la transformee de Fourrier
        R_a /= tf.to_float(M**2)
        R_a = tf.reduce_mean(R_a,axis=1)
        style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def loss_entropy(sess,net,image_style,M_dict,style_layers):
    """
    Computation of the entropy of the filters
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = (10**9)
    total_style_loss = 0.
    
    _, h_a, w_a, N = image_style.shape      
    sess.run(net['input'].assign(image_style))
        
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        
        a = tf.nn.l2_normalize(a, 3)
        x = tf.nn.l2_normalize(x, 3)
        
        entropy_a = tf.reduce_mean(-tf.multiply(a,tf.log(a)),axis=(0,1,2))
        entropy_x = tf.reduce_mean(-tf.multiply(x,tf.log(x)),axis=(0,1,2))
                
        style_loss = tf.nn.l2_loss(tf.subtract(entropy_x,entropy_a))  
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def compute_ImagePhaseAlea(sess,net,image_style,M_dict,style_layers): 
    """
    Add a random phase to the features of the image style at the last
    layer in the network
    """
    sess.run(net['input'].assign(image_style))
    image_style_PhaseAlea = {}
    layer, weight = style_layers[-1]
    a = sess.run(net[layer])
    b, h_a, w_a, N = a.shape
    at = tf.transpose(a, [0,3,1,2])
    F_a = tf.fft2d(tf.complex(at,0.))
    white_noise = np.random.normal(loc=0.0, scale=1.0, size=(h_a, w_a)).astype('float32')  # Le bruit blanc doit etre gaussien 
    F_white_noise = tf.fft2d(tf.complex(white_noise,0.))
    F_white_noise_modulus_inverse = tf.pow(tf.multiply(F_white_noise,tf.conj(F_white_noise)),-0.5)
    F_white_noise_phase = tf.multiply(F_white_noise,F_white_noise_modulus_inverse) # Respect Hermitian Symetry
    #F_white_noise_phase = np.ones((h_a, w_a))
    output_list = []

    for i in range(N):
        output_list.append(tf.multiply(F_a[0,i,:,:],F_white_noise_phase))

    F_a_new_phase = tf.stack(output_list)
    F_a_new_phase = tf.expand_dims(F_a_new_phase,axis=0)
    imF = tf.ifft2d(F_a_new_phase)
    imF =  tf.real(tf.transpose(imF, [0,2,3,1]))
    image_style_PhaseAlea[layer] = sess.run(imF)
    
    # Test pour verifier que le module de l image de style est bien respecter
    imF_t = tf.transpose(imF, [0,3,1,2])
    F_imF_t  = tf.fft2d(tf.complex(imF_t,0.))
    F_a_modulus = tf.real(tf.multiply(F_a,tf.conj(F_a)))
    F_a_modulus_num = sess.run(F_a_modulus)
    F_a_modulus_max = np.max(F_a_modulus_num)
    imF_modulus = tf.real(tf.multiply(F_imF_t,tf.conj(F_imF_t)))
    diff = tf.subtract(F_a_modulus,imF_modulus)
    diff = sess.run(diff)
    max_diff = np.max(diff)/F_a_modulus_max
    if(max_diff > 10**(-2)):
        print("The modulus of the image style features at the last layer is not preserved ! max_diff =",max_diff)
        raise Exception

    return(image_style_PhaseAlea)
    
def loss_PhaseAleatoire(sess,net,image_style,image_style_PhaseAlea,M_dict,style_layers,alpha=1,gamma_phaseAlea=1.):
    """
    In this loss function we impose the TF transform to the last layer 
    with a random phase imposed and only the spectrum of the others layers
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9
    total_style_loss = 0.
    last_style_layers,_ = style_layers[-1]
    for layer, weight in style_layers:
        if(last_style_layers==layer):
            N = style_layers_size[layer[:5]]
            M = M_dict[layer]
            x = net[layer]
            a_phase_alea = image_style_PhaseAlea[layer]
            loss = tf.nn.l2_loss(tf.subtract(x,a_phase_alea))
            loss *= gamma_phaseAlea * alpha *  weight * weight_help_convergence /(2.*(N**2)*tf.to_float(M**2)*length_style_layers)
            total_style_loss += loss
        else:
            sess.run(net['input'].assign(image_style))
            N = style_layers_size[layer[:5]]
            M = M_dict[layer]
            a = sess.run(net[layer])
            x = net[layer]
            x = tf.transpose(x, [0,3,1,2])
            a = tf.transpose(a, [0,3,1,2])
            F_x = tf.fft2d(tf.complex(x,0.))
            R_x = tf.real(tf.multiply(F_x,tf.conj(F_x))) # Module de la transformee de Fourrier : produit terme a terme
            R_x /= tf.to_float(M**2) # Normalisation du module de la TF
            F_a = tf.fft2d(tf.complex(a,0.))
            R_a = tf.real(tf.multiply(F_a,tf.conj(F_a))) # Module de la transformee de Fourrier
            R_a /= tf.to_float(M**2)
            style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
            style_loss *=  gamma_phaseAlea * weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
            total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def loss_PhaseAleatoireSimple(sess,net,image_style,image_style_PhaseAlea,M_dict,style_layers):
    """
    In this loss function we impose the TF transform to the last layer 
    with a random phase imposed that's all
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9
    total_style_loss = 0.
    last_style_layers,_ = style_layers[-1]
    alpha = 1
    for layer, weight in style_layers:
        if(last_style_layers==layer):
            N = style_layers_size[layer[:5]]
            M = M_dict[layer]
            x = net[layer]
            a_phase_alea = image_style_PhaseAlea[layer]
            loss = tf.nn.l2_loss(tf.subtract(x,a_phase_alea))
            loss *= alpha *  weight * weight_help_convergence /(2.*(N**2)*tf.to_float(M**2)*length_style_layers)
            total_style_loss += loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def compute_ImagePhaseAlea_list(sess,net,image_style,M_dict,style_layers): 
    """
    Add a random phase to the features of the image style at the last
    layer in the network
    """
    b, h, w, N = image_style.shape
    white_noise_img_bgr = np.random.normal(loc=0.0, scale=1.0, size=(h, w)).astype('float32')
    white_noise_img_bgr = np.stack((white_noise_img_bgr,white_noise_img_bgr,white_noise_img_bgr),axis=2)
    white_noise_img_bgr_tf = preprocess(white_noise_img_bgr)
    
    sess.run(net['input'].assign(white_noise_img_bgr_tf))
    
    dict = {}
    for layer,_ in style_layers:
        white_noise = sess.run(net[layer])
        white_noise = tf.transpose(white_noise, [0,3,1,2])
        F_white_noise = tf.fft2d(tf.complex(white_noise,0.))
        F_white_noise_modulus_inverse = tf.pow(tf.multiply(F_white_noise,tf.conj(F_white_noise)),-0.5)
        F_white_noise_phase = tf.multiply(F_white_noise,F_white_noise_modulus_inverse) # Respect Hermitian Symetry
        dict[layer] = F_white_noise_phase
        
    sess.run(net['input'].assign(image_style))
    
    image_style_PhaseAlea = {}
    for layer,_ in style_layers:
        a = sess.run(net[layer])
        at = tf.transpose(a, [0,3,1,2])
        F_a = tf.fft2d(tf.complex(at,0.))
        F_a_new_phase = tf.multiply(F_a,dict[layer])
        imF = tf.ifft2d(F_a_new_phase)
        imF =  tf.real(tf.transpose(imF, [0,2,3,1]))
        image_style_PhaseAlea[layer] = sess.run(imF)

    return(image_style_PhaseAlea)
    
def loss_PhaseAleatoirelist(sess,net,image_style,image_style_PhaseAlea,M_dict,style_layers):
    """
    In this loss function we impose the TF transform to the last layer 
    with a random phase imposed and only the spectrum of the 
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9
    total_style_loss = 0.
    alpha = 1
    print("Phase aleatoire List !")
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        x = net[layer]
        a_phase_alea = image_style_PhaseAlea[layer]
        loss = tf.nn.l2_loss(tf.subtract(x,a_phase_alea))
        loss *= alpha *  weight * weight_help_convergence /(2.*(N**2)*tf.to_float(M**2)*length_style_layers)
        total_style_loss += loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def loss_PhaseImpose(sess,net,image_style,M_dict,style_layers):
    """
    TODO !!!
    """
    #print("Here ")
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9
    total_style_loss = 0.
    last_style_layers,_ = style_layers[0]
    print(last_style_layers)
    sess.run(net['input'].assign(image_style))
    alpha = 10**(13)
    for layer, weight in style_layers:
        # contrainte sur le module uniquement 
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        x_t = tf.transpose(x, [0,3,1,2])
        a_t = tf.transpose(a, [0,3,1,2])
        F_x = tf.fft2d(tf.complex(x_t,0.))
        F_a = tf.fft2d(tf.complex(a_t,0.))
    
        if(last_style_layers==layer):
            ## Contrainte sur la phase 
            #angle_a = angle(F_a)
            ##angle_a_shiftdim1 = tf.concat([tf.expand_dims(angle_a[:,-1,:,:],0), angle_a[:,:-1,:,:]], axis=1)
            ##angle_a_prim = angle_a - angle_a_shiftdim1
            ##angle_a_shiftdim2 = tf.concat([tf.expand_dims(angle_a_prim[:,:,-1,:],axis=2), angle_a_prim[:,:,:-1,:]], axis=2)
            ##angle_a_prim = angle_a_prim - angle_a_shiftdim2
            ##angle_a_shiftdim3 = tf.concat([tf.expand_dims(angle_a_prim[:,:,:,-1],axis=3), angle_a_prim[:,:,:,:-1]], axis=3)
            ##angle_a_prim = angle_a_prim - angle_a_shiftdim3
            #angle_x = angle(F_x)
            ##angle_x_shiftdim1 = tf.concat([tf.expand_dims(angle_x[:,-1,:,:],0), angle_x[:,:-1,:,:]], axis=1)
            ##angle_x_prim = angle_x - angle_x_shiftdim1
            ##angle_x_shiftdim2 = tf.concat([tf.expand_dims(angle_x_prim[:,:,-1,:],axis=2), angle_x_prim[:,:,:-1,:]], axis=2)
            ##angle_x_prim = angle_x_prim - angle_x_shiftdim2
            ##angle_x_shiftdim3 = tf.concat([tf.expand_dims(angle_x_prim[:,:,:,-1],axis=3), angle_x_prim[:,:,:,:-1]], axis=3)
            ##angle_x_prim = angle_x_prim - angle_x_shiftdim3
            #angle_x /= tf.to_float(M)
            #angle_a /= tf.to_float(M)
            #style_loss = tf.nn.l2_loss(tf.subtract(angle_x,angle_a))   
            #style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
            #total_style_loss += style_loss
            
            # Correlation de phase ? 
            #innerProd = tf.multiply(F_x,tf.conj(F_a))  # sum(ftIm .* conj(ftRef), 3);
            #innerProd /= M**2
            #module_InnerProd = tf.pow(tf.real(tf.multiply(innerProd,tf.conj(innerProd))),0.5)
            #module_innerProd_less_1  = tf.pow(tf.pow(tf.real(tf.multiply(innerProd,tf.conj(innerProd)))-1,2),0.5)
            #style_loss = tf.reduce_sum(tf.multiply(module_InnerProd,module_innerProd_less_1))
            
            angle_x = tf.real(angle(F_x))
            angle_a = tf.real(angle(F_a))
            fft2_angle_x = tf.fft2d(tf.complex(angle_x,0.))
            fft2_angle_a = tf.fft2d(tf.complex(angle_a,0.))
            R_angle_x = tf.real(tf.multiply(fft2_angle_x,tf.conj(fft2_angle_x)))
            R_angle_a = tf.real(tf.multiply(fft2_angle_a,tf.conj(fft2_angle_a)))
            R_angle_a /= tf.to_float(M**2)
            R_angle_x /= tf.to_float(M**2)
            style_loss = tf.nn.l2_loss(tf.subtract(R_angle_x,R_angle_a))  
            
            #dephase = tf.divide(innerProd,module_InnerProd)
            #ftNew =  tf.multiply(dephase,F_x)
            #imF = tf.ifft2d(ftNew)
            #imF =  tf.real(tf.transpose(imF, [0,2,3,1]))
            #loss = tf.nn.l2_loss(tf.subtract(x,imF)) # sum (x**2)/2
            style_loss *= alpha* weight * weight_help_convergence /((2.*(N**2)*length_style_layers))
            total_style_loss += style_loss
        if True:
            R_x = tf.real(tf.multiply(F_x,tf.conj(F_x))) # Module de la transformee de Fourrier : produit terme a terme
            R_x /= tf.to_float(M) # Normalisation du module de la TF
            R_a = tf.real(tf.multiply(F_a,tf.conj(F_a))) # Module de la transformee de Fourrier
            R_a /= tf.to_float(M)
            style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a)) 
            style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
            total_style_loss += style_loss
            
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)    
    
def saveImExt(nameIm,nameOrign,extension,folder=''):
    nameNew = folder+  nameOrign + '_IntermediateIm_' + extension + '.png'
    copyfile(nameIm, nameNew)


def angle(z):
    """
    Returns the elementwise arctan of z, choosing the quadrant correctly.

    Quadrant I: arctan(y/x)
    Qaudrant II: π + arctan(y/x) (phase of x<0, y=0 is π)
    Quadrant III: -π + arctan(y/x)
    Quadrant IV: arctan(y/x)

    Inputs:
        z: tf.complex64 or tf.complex128 tensor
    Retunrs:
        Angle of z
    """
    if z.dtype == tf.complex128:
        dtype = tf.float64
    else:
        dtype = tf.float32
    x = tf.real(z)
    y = tf.imag(z)
    xneg = tf.cast(x < 0.0, dtype)
    yneg = tf.cast(y < 0.0, dtype)
    ypos = tf.cast(y >= 0.0, dtype)

    offset = xneg * (ypos - yneg) * np.pi

    return tf.atan(y / x) + offset

    
def loss_intercorr(sess,net,image_style,M_dict,style_layers):
    """
    Computation of the correlation of the filter and the interaction 
    long distance of the features : intercorrelation
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = (10**9)
    total_style_loss = 0.
    
    sess.run(net['input'].assign(image_style))  
    for layer, weight in style_layers:
        print(layer)
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        x = tf.transpose(x, [0,3,1,2])
        a = tf.transpose(a, [0,3,1,2])
        F_x = tf.fft2d(tf.complex(x,0.))
        F_x_conj = tf.conj(F_x)
        F_a = tf.fft2d(tf.complex(a,0.))
        F_a_conj = tf.conj(F_a)
        
        #NN = 2
        #alpha = 10
        #R_x = tf.real(tf.ifft2d(tf.multiply(F_x,F_x_conj)))
        #R_a = tf.real(tf.ifft2d(tf.multiply(F_a,F_a_conj)))
        #R_x /= tf.to_float(M**2)
        #R_a /= tf.to_float(M**2)
        #style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        #style_loss *=  alpha * weight * weight_help_convergence  / (2.*(NN**4)*length_style_layers)
        #total_style_loss += style_loss
        #lenRoll = sess.run(tf.random_uniform(minval=0,maxval=N,dtype=tf.int32,shape=[1])) # Between [minval,maxval)
        #print(lenRoll)
        #F_x = tf.concat([F_x[:,lenRoll:,:,:], F_x[:,:lenRoll,:,:]], axis=1)
        #F_a = tf.concat([F_a[:,lenRoll:,:,:], F_a[:,:lenRoll,:,:]], axis=1)
        #R_x = tf.real(tf.ifft2d(tf.multiply(F_x,F_x_conj)))
        #R_a = tf.real(tf.ifft2d(tf.multiply(F_a,F_a_conj)))
        #R_x /= tf.to_float(M**2)
        #R_a /= tf.to_float(M**2)
        #style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        #style_loss *=  weight * weight_help_convergence  / (2.*(NN**4)*length_style_layers)
        #total_style_loss += style_loss
        
        #lenRoll = sess.run(tf.random_uniform(minval=0,maxval=N,dtype=tf.int32,shape=[1]))
    
        #print(lenRoll)
        NN = N
        for i in range(NN):
            R_x = tf.real(tf.ifft2d(tf.multiply(F_x,F_x_conj)))
            R_a = tf.real(tf.ifft2d(tf.multiply(F_a,F_a_conj)))
            R_x /= tf.to_float(M**2)
            R_a /= tf.to_float(M**2)
            style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
            style_loss *=  weight * weight_help_convergence  / (2.*(NN**4)*length_style_layers)
            total_style_loss += style_loss
            #F_x = tf.stack([F_x[:,-1,:,:], F_x[:,:-1,:,:]], axis=1)
            F_x = tf.concat([tf.expand_dims(F_x[:,-1,:,:],0), F_x[:,:-1,:,:]], axis=1)
            #F_a = tf.stack([F_a[:,-1,:,:], F_a[:,:-1,:,:]], axis=1)
            F_a = tf.concat([tf.expand_dims(F_a[:,-1,:,:],0), F_a[:,:-1,:,:]], axis=1)
            
    return(total_style_loss)
    
def loss_SpectrumOnFeatures(sess,net,image_style,M_dict,style_layers):
    """
    In this loss function we impose the spectrum on each features 
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9
    total_style_loss = 0.
    sess.run(net['input'].assign(image_style))  
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        x = net[layer]
        x_transpose = tf.transpose(x, [0,3,1,2])
        a = tf.transpose(a, [0,3,1,2])
        F_x = tf.fft2d(tf.complex(x_transpose,0.))
        F_a = tf.fft2d(tf.complex(a,0.))
        innerProd = tf.multiply(F_x,tf.conj(F_a))  # sum(ftIm .* conj(ftRef), 3);
        module_InnerProd = tf.pow(tf.multiply(innerProd,tf.conj(innerProd)),0.5)
        dephase = tf.divide(innerProd,module_InnerProd)
        ftNew =  tf.multiply(dephase,F_x)
        imF = tf.ifft2d(ftNew)
        imF =  tf.real(tf.transpose(imF, [0,2,3,1]))
        loss = tf.nn.l2_loss(tf.subtract(x,imF)) # sum (x**2)/2
        loss *= weight * weight_help_convergence /(M*3*(2.*(N**2)*length_style_layers))
        total_style_loss += loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
    
def loss_fft3D(sess,net,image_style,M_dict,style_layers):
    """
    Computation of the 3-dimensional discrete Fourier Transform over the 
    inner-most 3 dimensions of input i.e. height,width,channel :) 
    """
    # TODO : change the M value attention !!! different size between a and x maybe 
    length_style_layers_int = len(style_layers)
    length_style_layers = float(length_style_layers_int)
    weight_help_convergence = 10**9
    total_style_loss = 0.
    x_temp = {}
    sess.run(net['input'].assign(image_style))  
    for layer, weight in style_layers:
        N = style_layers_size[layer[:5]]
        M = M_dict[layer]
        a = sess.run(net[layer])
        #R_a = (ifft2(fft2(a) * fft2(a).conj()).real)/M
        #R_x = x_temp[layer]
        x = net[layer]
        F_x = tf.fft3d(tf.complex(x,0.))
        #print(F_x.shape)
        R_x = tf.real(tf.multiply(F_x,tf.conj(F_x)))
        R_x /= tf.to_float(M**2)
        #print(R_x.shape)
        F_a = tf.fft3d(tf.complex(a,0.))
        R_a = tf.real(tf.multiply(F_a,tf.conj(F_a)))
        R_a /= tf.to_float(M**2)
        style_loss = tf.nn.l2_loss(tf.subtract(R_x,R_a))  
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
    total_style_loss =tf.to_float(total_style_loss)
    return(total_style_loss)
    
def loss_spectrum(sess,net,image_style,M_dict,beta,eps = 0.001):
    """
    Computation of an approximate version of the spectrum loss from Gang Liu 
    https://arxiv.org/pdf/1605.01141.pdf
    This can lead to translationResults
    """

    # TODO passer eps en argmument il est egal a 10**-16 sur matlab
    M = M_dict['input'] # Nombre de pixels
    
    x = net['input'] # Image en cours de synthese
        
    #For color images,
    #the phase of the gray level image is first computed, and then
    #imposed to each color channel
     
    a = tf.transpose(image_style, [0,3,1,2]) # On passe l image de batch,h,w,canaux à  batch,canaux,h,w
    F_a = tf.fft2d(tf.complex(a,0.)) # TF de l image de reference
    
    x_t = tf.transpose(x, [0,3,1,2])
    F_x = tf.fft2d(tf.complex(x_t,0.)) # Image en cours de synthese 
    
    # Element wise multiplication of FFT and conj of FFT
    if not(test_version_sup('1.8')):
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keep_dims=True)  # sum(ftIm .* conj(ftRef), 3);
    else:
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keepdims=True)  # sum(ftIm .* conj(ftRef), 3);
    # Shape = [  1   1 512 512] pour une image 512*512
    #module_InnerProd = tf.pow(tf.multiply(innerProd,tf.conj(innerProd)),0.5) # replace by tf.abs
    #print(innerProd)
    #if tf.__version__ > '1.4':
        #module_InnerProd = tf.complex(tf.abs(innerProd),0.) # Possible with tensorflow 1.4
    #else:
    module_InnerProd = tf.pow(tf.multiply(innerProd,tf.conj(innerProd)),0.5)
    #print(module_InnerProd)
    dephase = tf.divide(innerProd,tf.add(module_InnerProd,eps))
    #print(dephase)
    ftNew =  tf.multiply(dephase,F_a) #compute the new version of the FT of the reference image
    # Element wise multiplication
    imF = tf.ifft2d(ftNew)
    imF =  tf.real(tf.transpose(imF, [0,2,3,1]))
    loss = tf.nn.l2_loss(tf.subtract(x,imF)) # sum (x**2)/2
    loss *= beta/(3*M)
    return(loss)  
    
def loss_spectrumtest(sess,net,image_style,M_dict,beta,eps = 0.001):
    """
    Computation of an approximate version of the spectrum loss from Gang Liu 
    https://arxiv.org/pdf/1605.01141.pdf
    This can lead to translationResults 
    Fonction pour faire des tests 
    """
    #eps = 0.
    #eps = 0.001
    M = M_dict['input'] # Nombre de pixels
    
    x = net['input'] # Image en cours de synthese
        
    #For color images,
    #the phase of the gray level image is first computed, and then
    #imposed to each color channel
     
    a = tf.transpose(image_style, [0,3,1,2]) # On passe l image de batch,h,w,canaux à  batch,canaux,h,w
    F_a = tf.fft2d(tf.complex(a,0.)) # TF de l image de reference
    
    x_t = tf.transpose(x, [0,3,1,2])
    F_x = tf.fft2d(tf.complex(x_t,0.)) # Image en cours de synthese 
    
    # Element wise multiplication of FFT and conj of FFT
    if not(test_version_sup('1.8')):
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keep_dims=True)  # sum(ftIm .* conj(ftRef), 3); cad somme sur les channels 
    else:
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keepdims=True)  # sum(ftIm .* conj(ftRef), 3); cad somme sur les channels 
    # Shape = [  1   1 512 512] pour une image 512*512
    #module_InnerProd = tf.pow(tf.multiply(innerProd,tf.conj(innerProd)),0.5) # replace by tf.abs
    #print(innerProd)
    #if tf.__version__ > '1.4':
        #module_InnerProd = tf.complex(tf.abs(innerProd),0.) # Possible with tensorflow 1.4
    #else:
    #module_InnerProd = tf.pow(tf.multiply(innerProd,tf.conj(innerProd)),0.5)
    #product = tf.multiply(innerProd,tf.conj(innerProd))
    product = tf.add(tf.pow(tf.real(innerProd),2.),tf.pow(tf.imag(innerProd),2.)) # cela semble etre ce qui cause la synthese translation !
    # Mais en fait non parce que l on peut obtenir la translation avec le code loss_spectrumTFabs
    #product = tf.add(tf.pow(tf.real(innerProd),2.),tf.pow(tf.imag(innerProd),2)) # Mais pas celle la
    product_real = tf.real(product)
    module_InnerProd = tf.complex(tf.sqrt(product_real),0.)
    #print(module_InnerProd)
    dephase = tf.divide(innerProd,tf.add(module_InnerProd,eps))
    #print(dephase)
    ftNew =  tf.multiply(dephase,F_a) #compute the new version of the FT of the reference image
    # Element wise multiplication
    imF = tf.ifft2d(ftNew)
    imF =  tf.real(tf.transpose(imF, [0,2,3,1]))
    loss = tf.nn.l2_loss(tf.subtract(x,imF)) # sum (x**2)/2
    loss *= beta/(3*M)
    return(loss)  
      
def loss_spectrumTFabs(sess,net,image_style,M_dict,beta,eps = 0.001):
    """
    Computation of the spectrum loss from Gang Liu 
    https://arxiv.org/pdf/1605.01141.pdf
    enfin une reimplementation qui a l air de faire aussi des translations :/
    """
    #eps = 10**(-16)
    #
    print('eps value :',eps)
    M = M_dict['input'] # Nombre de pixels
    
    x = net['input'] # Image en cours de synthese
        
    #For color images,
    #the phase of the gray level image is first computed, and then
    #imposed to each color channel
     
    a = tf.transpose(image_style, [0,3,1,2]) # On passe l image de batch,h,w,canaux à  batch,canaux,h,w
    F_a = tf.fft2d(tf.complex(a,0.)) # TF de l image de reference
    
    x_t = tf.transpose(x, [0,3,1,2])
    F_x = tf.fft2d(tf.complex(x_t,0.)) # Image en cours de synthese 
    
    # Element wise multiplication of FFT and conj of FFT
    if not(test_version_sup('1.8')):
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keep_dims=True)  # sum(ftIm .* conj(ftRef), 3); cad somme sur les channels 
    else:
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keepdims=True)  # sum(ftIm .* conj(ftRef), 3); cad somme sur les channels 
    # Shape = [  1   1 512 512] pour une image 512*512
    #module_InnerProd = tf.pow(tf.multiply(innerProd,tf.conj(innerProd)),0.5) # replace by tf.abs
    #print(innerProd)
    if test_version_sup('1.4'):
        module_InnerProd = tf.complex(tf.abs(innerProd),0.) # Possible with tensorflow 1.4
    else:
        raise(NotImplemented)
    #print(module_InnerProd)
    dephase = tf.divide(innerProd,tf.add(module_InnerProd,eps))
    #print(dephase)
    ftNew =  tf.multiply(dephase,F_a) #compute the new version of the FT of the reference image
    # Element wise multiplication
    imF = tf.ifft2d(ftNew)
    imF =  tf.real(tf.transpose(imF, [0,2,3,1]))
    loss = tf.nn.l2_loss(tf.subtract(x,imF)) # sum (x**2)/2
    loss *= beta/(3*M)
    return(loss)    
    
@tf.custom_gradient   
def loss_spectrumTFabs_WithGrad(sess,net,image_style,M_dict,beta,eps = 0.001):
    """
    Computation of the spectrum loss from Gang Liu 
    https://arxiv.org/pdf/1605.01141.pdf
    enfin une reimplementation qui a l air de faire aussi des translations :/
    En cours de test 
    """
    #eps = 10**(-16)
    #
    M = M_dict['input'] # Nombre de pixels
    
    x = net['input'] # Image en cours de synthese
        
    #For color images,
    #the phase of the gray level image is first computed, and then
    #imposed to each color channel
     
    a = tf.transpose(image_style, [0,3,1,2]) # On passe l image de batch,h,w,canaux à  batch,canaux,h,w
    F_a = tf.fft2d(tf.complex(a,0.)) # TF de l image de reference
    
    x_t = tf.transpose(x, [0,3,1,2])
    F_x = tf.fft2d(tf.complex(x_t,0.)) # Image en cours de synthese 
    
    # Element wise multiplication of FFT and conj of FFT
    if not(test_version_sup('1.8')):
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keep_dims=True)  # sum(ftIm .* conj(ftRef), 3); cad somme sur les channels 
    else:
        innerProd = tf.reduce_sum(tf.multiply(F_x,tf.conj(F_a)), 1, keepdims=True)  # sum(ftIm .* conj(ftRef), 3); cad somme sur les channels 
    # Shape = [  1   1 512 512] pour une image 512*512
    #module_InnerProd = tf.pow(tf.multiply(innerProd,tf.conj(innerProd)),0.5) # replace by tf.abs
    #print(innerProd)
    if test_version_sup('1.4'):
        module_InnerProd = tf.complex(tf.abs(innerProd),0.) # Possible with tensorflow 1.4
    else:
        raise(NotImplemented)
    #print(module_InnerProd)
    dephase = tf.divide(innerProd,tf.add(module_InnerProd,eps))
    #print(dephase)
    ftNew =  tf.multiply(dephase,F_a) #compute the new version of the FT of the reference image
    # Element wise multiplication
    imF = tf.ifft2d(ftNew)
    imF_transposeback =  tf.real(tf.transpose(imF, [0,2,3,1]))
    loss = tf.nn.l2_loss(tf.subtract(x,imF_transposeback)) # sum (x**2)/2
    loss *= beta/(3*M)
    
    def grad(dy):
        diff_im = tf.subtract(x,imF_transposeback)
        grad_custom = dy * tf.multiply(diff_im,beta/(3*M))
        return grad_custom

    return(loss,grad)    

def loss__HF_filter(sess, net, image_style,M_dict):
    """
    With the function we add a kind of HF Filter to some layer in order 
    to remove some artefacts
    """
    weight_help_convergence = 10**(9) # This wight come from a paper of Gatys 
    sizeKernel = 3
    zeros = np.zeros(shape=(sizeKernel,sizeKernel)).astype('float32')
    k = np.array([[ 1, -1,  1],
       [-1,  1, -1],
       [ 1, -1,  1]]).astype('float32')
    k /= 9.
    # imshow(abs(fftshift(fft2(a,256,256))),[]) pour connaitre le spectre sous matlab
    total_style_loss = 0.
    
    style_layers_local = [('input',1),('pool1',1),('pool2',1),('pool3',1),('pool4',1)]
    length_style_layers = len(style_layers_local)
    style_layers_local_size = {'input' : 3,'pool1' : 64,'pool2' : 128,'pool3' : 256,'pool4' : 512}
    for layer, weight in style_layers_local:
        weight =1.0
        input_x  = net[layer]
        kernels = None
        for j in range(style_layers_local_size[layer]):
            kernels_tmp = np.zeros(shape=(sizeKernel,sizeKernel,style_layers_local_size[layer])).astype('float32')
            kernels_tmp[:,:,j] = k #/math.sqrt(sizeKernel*sizeKernel*style_layers_local_size[layer])
            kernels_tmp = np.expand_dims(kernels_tmp,axis=3)
            if(kernels is None):
                kernels = kernels_tmp 
            else:
                kernels = np.concatenate((kernels,kernels_tmp),axis=3)
        _,_,_,N = kernels.shape  
        kernels = tf.constant(kernels)

        conv_x = tf.nn.conv2d(input_x, kernels, strides=(1, 1, 1, 1),
                padding='SAME',name='conv')
        sess.run(net['input'].assign(image_style)) 
        input_a = sess.run(net[layer])
        conv_a = tf.nn.conv2d(input_a, kernels, strides=(1, 1, 1, 1),
                padding='SAME',name='conv')
        M = M_dict[layer]
        G_x = gram_matrix(conv_x,N,M)
        G_a = gram_matrix(conv_a,N,M)
        style_loss = tf.nn.l2_loss(tf.subtract(G_x,G_a))
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
        
    return(total_style_loss)

def loss__HF_many_filters2(sess, net, image_style,M_dict):
    """
    test avec d'autres filtres passe haut
    """
    weight_help_convergence = 10**(9) # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    zeros = np.zeros(shape=(3,3)).astype('float32')
    
    #Filter en X
    list_kernel = []
    k = np.array([[ 1, -1,  1],
       [-1,  1, -1],
       [ 1, -1,  1]]).astype('float32')
    k = k/9.
    list_kernel += [k]
    # Filtre selon x
    k = np.array([[ -1, 1,  0],
       [-1,  1, 0],
       [ -1, 1,  0]]).astype('float32')
    k = k/9.
    list_kernel += [k]
    # Filtre selon y
    k = np.array([[ 0, 0,  0],
       [1,  1, 1],
       [ -1, -1,  -1]]).astype('float32')
    k = k/9.
    list_kernel += [k]
    # Filtre Laplacien
    k = np.array([[ 0, -1/4,  0],
       [-1./4.,  1, -1./4.],
       [ 0, -1./4.,  0]]).astype('float32')
    k = k/9.
    list_kernel += [k]
    #k = np.array([[ 0, -1./4.,  0],
        #[-1./4,  2, -1./4],
       #[ 0, -1./4,  0]]).astype('float32')
    #k = k/9.
    ratio = 1./len(list_kernel)
    weight = 10**(-2)
    style_layers_local = [('input',1),('pool1',1),('pool2',1),('pool3',1),('pool4',1)]
    length_style_layers = len(style_layers_local)
    style_layers_local_size = {'input' : 3,'pool1' : 64,'pool2' : 128,'pool3' : 256,'pool4' : 512}
    for layer, weight in style_layers_local:
        for kernel in list_kernel:
            k = kernel
            input_x  = net[layer]
            kernels = None
            for j in range(style_layers_local_size[layer]):
                kernels_tmp = np.zeros(shape=(3,3,style_layers_local_size[layer])).astype('float32')
                kernels_tmp[:,:,j] = k
                kernels_tmp = np.expand_dims(kernels_tmp,axis=3)
                if(kernels is None):
                    kernels = kernels_tmp 
                else:
                    kernels = np.concatenate((kernels,kernels_tmp),axis=3)
            _,_,_,N = kernels.shape  
            kernels = tf.constant(kernels)

            conv_x = tf.nn.conv2d(input_x, kernels, strides=(1, 1, 1, 1),
                    padding='SAME',name='conv')
            sess.run(net['input'].assign(image_style)) 
            input_a = sess.run(net[layer])
            conv_a = tf.nn.conv2d(input_a, kernels, strides=(1, 1, 1, 1),
                    padding='SAME',name='conv')
            M = M_dict[layer]
            G_x = gram_matrix(conv_x,N,M)
            G_a = gram_matrix(conv_a,N,M)
            style_loss = tf.nn.l2_loss(tf.subtract(G_x,G_a))
            style_loss *= ratio* weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
            total_style_loss += style_loss
    return(total_style_loss)
    
    
def loss__HF_many_filters(sess, net, image_style,M_dict):
    weight_help_convergence = 10**(9) # This wight come from a paper of Gatys
    # Because the function is pretty flat 
    total_style_loss = 0
    sizeKernel = 3
    zeros = np.zeros(shape=(sizeKernel,sizeKernel)).astype('float32')
    k = np.array([[ 1, -1,  1],
       [-1,  1, -1],
       [ 1, -1,  1]]).astype('float32')
    #k /= 9.
    
    style_layers_local = [('input',1),('pool1',1),('pool2',1),('pool3',1),('pool4',1)]
    length_style_layers = len(style_layers_local)
    style_layers_local_size = {'input' : 3,'pool1' : 64,'pool2' : 128,'pool3' : 256,'pool4' : 512}
    for layer, weight in style_layers_local:
        weight =1.0
        input_x  = net[layer]
        kernels = None
        for j in range(style_layers_local_size[layer]):
            kernels_tmp = np.zeros(shape=(sizeKernel,sizeKernel,style_layers_local_size[layer])).astype('float32')
            kernels_tmp[:,:,j] = k/math.sqrt(sizeKernel*sizeKernel*style_layers_local_size[layer])
            kernels_tmp = np.expand_dims(kernels_tmp,axis=3)
            if(kernels is None):
                kernels = kernels_tmp 
            else:
                kernels = np.concatenate((kernels,kernels_tmp),axis=3)
        _,_,_,N = kernels.shape  
        kernels = tf.constant(kernels)

        conv_x = tf.nn.conv2d(input_x, kernels, strides=(1, 1, 1, 1),
                padding='SAME',name='conv')
        sess.run(net['input'].assign(image_style)) 
        input_a = sess.run(net[layer])
        conv_a = tf.nn.conv2d(input_a, kernels, strides=(1, 1, 1, 1),
                padding='SAME',name='conv')
        M = M_dict[layer]
        G_x = gram_matrix(conv_x,N,M)
        G_a = gram_matrix(conv_a,N,M)
        style_loss = tf.nn.l2_loss(tf.subtract(G_x,G_a))
        style_loss *=  weight * weight_help_convergence  / (2.*(N**2)*length_style_layers)
        total_style_loss += style_loss
        
        if(layer=='input'):
            weight2 = 10**(-5)
            k2 = np.ones(shape=(sizeKernel,sizeKernel)).astype('float32')
            k2 /= 9.
            kernels = None
            for j in range(style_layers_local_size[layer]):
                kernels_tmp = np.zeros(shape=(sizeKernel,sizeKernel,style_layers_local_size[layer])).astype('float32')
                kernels_tmp[:,:,j] = k2
                kernels_tmp[:,:,(j+1)%(style_layers_local_size[layer])] = -k2
                kernels_tmp = np.expand_dims(kernels_tmp,axis=3)
                if(kernels is None):
                    kernels = kernels_tmp 
                else:
                    kernels = np.concatenate((kernels,kernels_tmp),axis=3)
            _,_,_,N = kernels.shape
            kernels = tf.constant(kernels)

            conv_x = tf.nn.conv2d(input_x, kernels, strides=(1, 1, 1, 1),
                    padding='SAME',name='conv')
            sess.run(net['input'].assign(image_style)) 
            input_a = sess.run(net[layer])
            conv_a = tf.nn.conv2d(input_a, kernels, strides=(1, 1, 1, 1),
                    padding='SAME',name='conv')
            M = M_dict[layer]
            G_x = gram_matrix(conv_x,N,M)
            G_a = gram_matrix(conv_a,N,M)
            style_loss = tf.nn.l2_loss(tf.subtract(G_x,G_a))
            style_loss *=  weight2 * weight_help_convergence  / (2.*(N**2)*length_style_layers)
            total_style_loss += style_loss
    return(total_style_loss)

def sum_total_variation_losses(sess, net,alpha_TV):
    """
    denoising loss function, this function come from : 
    https://github.com/cysmith/neural-style-tf/blob/master/neural_style.py
    this version of the total variation is  anisotropy.
    Here we have loss = (norm_2 (grad_x(I)) + norm_2 (grad_y(I)))^2
    
    In Jonhson we have : https://github.com/jcjohnson/neural-style/blob/master/neural_style.lua
    -- TV loss backward pass inspired by kaishengtai/neuralart
    function TVLoss:updateGradInput(input, gradOutput)
      self.gradInput:resizeAs(input):zero()
      local C, H, W = input:size(1), input:size(2), input:size(3)
      self.x_diff:resize(3, H - 1, W - 1)
      self.y_diff:resize(3, H - 1, W - 1)
      self.x_diff:copy(input[{{}, {1, -2}, {1, -2}}])
      self.x_diff:add(-1, input[{{}, {1, -2}, {2, -1}}])
      self.y_diff:copy(input[{{}, {1, -2}, {1, -2}}])
      self.y_diff:add(-1, input[{{}, {2, -1}, {1, -2}}])
      self.gradInput[{{}, {1, -2}, {1, -2}}]:add(self.x_diff):add(self.y_diff)
      self.gradInput[{{}, {1, -2}, {2, -1}}]:add(-1, self.x_diff)
      self.gradInput[{{}, {2, -1}, {1, -2}}]:add(-1, self.y_diff)
      self.gradInput:mul(self.strength)
      self.gradInput:add(gradOutput)
      return self.gradInput
    end
    Inspired from https://github.com/kaishengtai/neuralart/blob/297112666def04397a00b178eb77172df607dda6/costs.lua
    
    """
    x = net['input']
    weight_help_convergence = 10**9
    #alpha = 10**(-6) # In order to not destroy the image to a constance 
    # total variation regularization  with a strength of between 1 × 10 −6 and 1 × 10 −4 from  Jonshon
    [b, h, w, d] = x.get_shape()
    b, h, w, d = tf.to_int32(b),tf.to_int32(h),tf.to_int32(w),tf.to_int32(d)
    tv_y_size = tf.to_float(b * (h-1) * w * d) # Nombre de pixels
    tv_x_size = tf.to_float(b * h * (w-1) * d)
    loss_y = tf.nn.l2_loss(x[:,1:,:,:] - x[:,:-1,:,:]) 
    loss_y /= tv_y_size
    loss_x = tf.nn.l2_loss(x[:,:,1:,:] - x[:,:,:-1,:]) 
    loss_x /= tv_x_size
    loss = 2 *alpha_TV * weight_help_convergence * (loss_y + loss_x)
    loss = tf.cast(loss, tf.float32)
    return(loss)
    
def sum_TV_ronde_losses(sess, net,alpha_TVronde):
    """
    denoising loss function, 
    this version of the total variation is  anisotropy.
    Here we have loss = sum sqrt((grad_x(I)^2) + (grad_y(I)^2))
    """
    x = net['input']
    weight_help_convergence = 10*9
    grad_x = x[:,1:,:,:] - x[:,:-1,:,:]
    grad_x_padding = tf.constant([[0,0],[0,1],[0,0],[0,0]])
    grad_x = tf.pad(grad_x,grad_x_padding,mode='CONSTANT',constant_values=0)
    grad_y =x[:,:,1:,:] - x[:,:,:-1,:]
    grad_y_padding = tf.constant([[0,0],[0,0],[0,1],[0,0]])
    grad_y = tf.pad(grad_y,grad_y_padding,mode='CONSTANT',constant_values=0)
    ampl_grad = tf.pow(tf.add(tf.add(tf.pow(grad_x,2),tf.pow(grad_y,2)),1.),0.5)
    loss = 2 *alpha_TVronde * weight_help_convergence * tf.pow(tf.reduce_mean(ampl_grad),2)
    loss = tf.cast(loss, tf.float32)
    return(loss)

def sum_total_variation_TF(sess, net):
    """
    Autre possibilité pour arriver au meme que sum_total_variation_losses
    https://www.tensorflow.org/api_docs/python/tf/image/total_variation
    """
    x = net['input']
    weight_help_convergence = 1.
    alpha = 10**(-6) # In order to not destroy the image to a constance 
    loss = weight_help_convergence* alpha * tf.reduce_sum(tf.image.total_variation(x))
    return(loss)
    
    
def sum_total_variation_losses_norm1(sess, net,alpha_TV):
    """
    denoising loss function, this function come from : 
    https://github.com/cysmith/neural-style-tf/blob/master/neural_style.py
    
    TV anisotropique sum of absolute value of the gradient
    """
    x = net['input']
    weight_help_convergence = 10**9
    loss_y = tf.reduce_mean(tf.abs(x[:,1:,:,:] - x[:,:-1,:,:]))
    loss_x = tf.reduce_mean(tf.abs(x[:,:,1:,:] - x[:,:,:-1,:]))
    loss = alpha_TV * weight_help_convergence * (loss_y + loss_x)
    loss = tf.cast(loss, tf.float32)
    return(loss)
        
def gram_matrix(x,N,M):
  """
  Computation of the Gram Matrix for one layer we normalize with the 
  number of pixels M
  
  Warning the way to compute the Gram Matrix is different from the paper
  but it is equivalent, we use here the F matrix with the shape M*N
  That's quicker
  """
  # The implemented version is quicker than this one :
  #x = tf.transpose(x,(0,3,1,2))
  #F = tf.reshape(x,[tf.to_int32(N),tf.to_int32(M)])
  #G = tf.matmul(F,tf.transpose(F))
  
  F = tf.reshape(x,[M,N])
  G = tf.matmul(tf.transpose(F),F)
  G /= tf.to_float(M)
  # That come from Control paper
  return(G)
 
def get_Gram_matrix(vgg_layers,image_style,pooling_type='avg',padding='SAME',args=None):
    """
    Computation of all the Gram matrices from one image thanks to the 
    vgg_layers
    """
    if args.GramLightComput:
        print('Warning you are using a light version of the Gram Matrix dictionnary,this can cause some side effect it was not test !')
        # TODO : tester tous les cas limites genre les configurations etc !
        layers_to_use = []
        layers_to_use += args.style_layers 
        if 'content' in args.loss or 'full' in args.loss:
            layers_to_use += args.content_layers 
    else: 
        layers_to_use = VGG19_LAYERS 
    
    dict_gram = {}
    net = net_preloaded(vgg_layers, image_style,pooling_type,padding) # net for the style image
    sess = tf.Session()
    sess.run(net['input'].assign(image_style))
    a = net['input']
    _,height,width,N = a.shape
    M = height*width
    A = gram_matrix(a,tf.to_int32(N),tf.to_int32(M)) #  TODO Need to divided by M ????
    dict_gram['input'] = sess.run(A)    
    for layer in layers_to_use:
        a = net[layer]
        _,height,width,N = a.shape
        M = height*width
        A = gram_matrix(a,tf.to_int32(N),tf.to_int32(M)) #  TODO Need to divided by M ????
        dict_gram[layer] = sess.run(A) # Computation
    sess.close()
    tf.reset_default_graph() # To clear all operation and variable
    return(dict_gram)        
         
def get_features_repr(vgg_layers,image_content,pooling_type='avg',padding='SAME'):
    """
    Compute the image content representation values according to the vgg
    19 net
    """
    net = net_preloaded(vgg_layers, image_content,pooling_type,padding) # net for the content image
    sess = tf.Session()
    sess.run(net['input'].assign(image_content))
    dict_features_repr = {}
    for layer in VGG19_LAYERS:
        P = sess.run(net[layer])
        dict_features_repr[layer] = P # Computation
    sess.close()
    tf.reset_default_graph() # To clear all operation and variable
    return(dict_features_repr)  
    
def get_clip_values(image_style=None,BGR=False):
    """
    Return the max and min value for the clipping and other find in the
    optimization
    If BGR is True then we return 2 vectors of 3 elements
    """
    if(image_style is None):
        # Max et min value from the ImageNet databse mean
        clip_value_min=-124.
        clip_value_max=152.
    elif(BGR==False):
        clip_value_max = np.max(image_style)
        clip_value_min = np.min(image_style)
    elif(BGR==True):
        clip_value_min = np.min(image_style,axis=(0,1,2)) # along the BGR axis
        #print(clip_value_min)
        clip_value_max = np.max(image_style,axis=(0,1,2)) # along the BGR axis
        #print(clip_value_max)
        #print(np.max(image_style[:,:,:,0]),np.max(image_style[:,:,:,1]),np.max(image_style[:,:,:,2]))
    return(clip_value_min,clip_value_max)
    
def preprocess(img):
    """
    This function takes a RGB image and process it to be used with 
    tensorflow
    """
    # shape (h, w, d) to (1, h, w, d)
    img = img[np.newaxis,:,:,:]
    img -= np.array([103.939, 116.779,123.68 ]).reshape((1,1,1,3))
    ## subtract the imagenet mean for a RGB image
    #img -= np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3)) # In order to have channel = (channel - mean) / std with std = 1
    ## The input images should be zero-centered by mean pixel (rather than mean image) 
    ## subtraction. Namely, the following BGR values should be subtracted: [103.939, 116.779, 123.68].
    ## From https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md
    #try:
        #img = img
        ##img = img[...,::-1] # rgb to bgr # In the Gang weight case the image are RGB 
    #except IndexError:
        #raise
    # Both VGG-16 and VGG-19 were trained using Caffe, and Caffe uses OpenCV to 
    # load images which uses BGR by default, so both VGG models are expecting BGR images.
    # It is the case for the .mat save we are using here.
    
    return(img)

def postprocess(img):
    """
    To the unprocessing analogue to the "preprocess" function from 4D array
    to RGB image
    """
    # bgr to rgb
    #img = img[...,::-1]
    # add the imagenet mean 
    #img += np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))

    img += np.array([103.939, 116.779,123.68 ]).reshape((1,1,1,3))
    # shape (1, h, w, d) to (h, w, d)
    img = img[0]
    img = np.clip(img,0,255).astype('uint8')
    return(img)  

def get_M_dict(image_h,image_w):
    """
    This function compute the size of the different dimension in the con
    volutionnal net
    """
    M_dict =  {'conv1' : 0,'relu1' : 0,'pool1':0,'conv2' : 0,'relu2' : 0,'pool2':0,'conv3' : 0,'relu3' : 0,'pool3':0,'conv4': 0,'relu4' : 0,'pool4':0,'conv5' : 0,'relu5' : 0,'pool5':0}
    M_dict = {
    'conv1_1': 0, 'relu1_1': 0, 'conv1_2': 0, 'relu1_2': 0, 'pool1': 0,

    'conv2_1': 0, 'relu2_1': 0, 'conv2_2': 0, 'relu2_2': 0, 'pool2': 0,

    'conv3_1': 0, 'relu3_1': 0, 'conv3_2': 0, 'relu3_2': 0, 'conv3_3': 0,
    'relu3_3': 0, 'conv3_4': 0, 'relu3_4': 0, 'pool3': 0,

    'conv4_1': 0, 'relu4_1': 0, 'conv4_2': 0, 'relu4_2': 0, 'conv4_3': 0,
    'relu4_3': 0, 'conv4_4': 0, 'relu4_4': 0, 'pool4': 0,

    'conv5_1': 0, 'relu5_1': 0, 'conv5_2': 0, 'relu5_2': 0, 'conv5_3': 0,
    'relu5_3': 0, 'conv5_4': 0, 'relu5_4': 0}
    image_h_tmp = image_h
    image_w_tmp = image_w
    M = image_h_tmp*image_w_tmp
    for key in M_dict.keys():
        if(key[:4]=='conv'):
            M_dict[key] = M
        elif(key[:4]=='pool'):
            image_h_tmp =  math.ceil(image_h_tmp / 2)
            image_w_tmp = math.ceil(image_w_tmp / 2)
            M = image_h_tmp*image_w_tmp
            M_dict[key] = M
        elif(key[:4]=='relu'):
            M_dict[key] = M
    M_dict['input'] = M_dict['conv1_1']
    return(M_dict)  
    
def get_M_dict_Davy(image_h,image_w):
    """
    This function compute the size of the different dimension in the con
    volutionnal net when using a VALID padding without circular extensio
    """
    M_dict = {
    'conv1_1': 0, 'relu1_1': 0, 'conv1_2': 0, 'relu1_2': 0, 'pool1': 0,

    'conv2_1': 0, 'relu2_1': 0, 'conv2_2': 0, 'relu2_2': 0, 'pool2': 0,

    'conv3_1': 0, 'relu3_1': 0, 'conv3_2': 0, 'relu3_2': 0, 'conv3_3': 0,
    'relu3_3': 0, 'conv3_4': 0, 'relu3_4': 0, 'pool3': 0,

    'conv4_1': 0, 'relu4_1': 0, 'conv4_2': 0, 'relu4_2': 0, 'conv4_3': 0,
    'relu4_3': 0, 'conv4_4': 0, 'relu4_4': 0, 'pool4': 0,

    'conv5_1': 0, 'relu5_1': 0, 'conv5_2': 0, 'relu5_2': 0, 'conv5_3': 0,
    'relu5_3': 0, 'conv5_4': 0, 'relu5_4': 0}
    
    image_h_tmp = image_h
    image_w_tmp = image_w
    M = image_h_tmp*image_w_tmp
    for key in M_dict.keys():
        if(key[:4]=='conv'):
            image_h_tmp = image_h_tmp - 2
            image_w_tmp = image_w_tmp - 2
            M  = image_h_tmp*image_w_tmp
            M_dict[key] = M
        elif(key[:4]=='pool'):
            image_h_tmp =  math.ceil((image_h_tmp-1) / 2)
            image_w_tmp = math.ceil((image_w_tmp-1) / 2)
            M = image_h_tmp*image_w_tmp
            M_dict[key] = M
        elif(key[:4]=='relu'):
            M_dict[key] = M
    M_dict['input'] = M_dict['conv1_1']
    return(M_dict)  
    
def print_loss_tab(sess,list_loss,list_loss_name):
    """
    Fonction pour afficher la valeur des différentes loss
    """
    strToPrint = ''
    for loss,loss_name in zip(list_loss,list_loss_name):
        loss_tmp = sess.run(loss)
        strToPrint +=  loss_name + ' = {:.2e}, '.format(loss_tmp)
    print(strToPrint)
    
def print_loss(sess,loss_total,content_loss,style_loss):
    loss_total_tmp = sess.run(loss_total)
    content_loss_tmp = sess.run(content_loss)
    style_loss_tmp = sess.run(style_loss)
    strToPrint ='Total loss = {:.2e}, Content loss  = {:.2e}, Style loss  = {:.2e}'.format(loss_total_tmp,content_loss_tmp,style_loss_tmp)
    print(strToPrint)

def get_init_noise_img(image_content,init_noise_ratio,range_value):
    """ This function return a white noise image for the initialisation 
    this image can be linearly miwed with the image content with a ratio
    """
    _,image_h, image_w, number_of_channels = image_content.shape 
    low = 127.5 - range_value
    high = 127.5 + range_value
    noise_img = np.random.uniform(low,high, (image_h, image_w, number_of_channels)).astype('float32')
    noise_img = preprocess(noise_img)
    if(init_noise_ratio >= 1.):
        noise_img = noise_img
    elif(init_noise_ratio <= 0.0):
        noise_img = image_content
    else:
        noise_img = init_noise_ratio* noise_img + (1.-init_noise_ratio) * image_content
    return(noise_img)
    
def get_init_noise_img_smooth_grad(image_content):
    """
    This function return a random initial image with a mean near to the 
    mean value of the content image and with a smooth gradient 
    """
    from skimage import filters
    _,image_h, image_w, number_of_channels = image_content.shape 
    low = -1
    high = 1
    noise_img = np.random.uniform(low,high, (image_h, image_w, number_of_channels))
    gaussian_noise_img = filters.gaussian(noise_img, sigma=2,mode='reflect')
    for i in range(3):
         gaussian_noise_img[:,:,i] += np.mean(image_content[:,:,i]) # Add the mean of each channel
    gaussian_noise_img = np.clip(gaussian_noise_img,0.,255.)
    preprocess_img = preprocess(gaussian_noise_img)
    return(preprocess_img)
    
def get_init_noise_img_gaussian(image_content,init_noise_ratio):
    """
    Generate an image with a gaussian white noise aroud the BGR mean of the
    image content
    """
    b,image_h, image_w, number_of_channels = image_content.shape 
    noise_img = np.random.randn(b,image_h, image_w, number_of_channels) 
    bgr_mean = np.mean(image_content,axis=(0,1,2)) # This mean have been recentered by the ImageNet mean
    for i in range(number_of_channels):
        noise_img[:,:,:,i] += bgr_mean[i]
    # random floats sampled from a univariate “normal” (Gaussian) distribution of mean 0 and variance 1 
    # Doesn't need preprocess because already arond 0 with a small range
    
    if(init_noise_ratio >= 1.):
        noise_img = noise_img
    elif(init_noise_ratio <= 0.0):
        noise_img = image_content
    else:
        noise_img = init_noise_ratio* noise_img + (1.-init_noise_ratio) * image_content
    
    return(noise_img)
    
def get_lbfgs_bnds_tf_1_2(init_img,clip_value_min,clip_value_max,BGR=False):
    """
    This function create the bounds for the LBFGS scipy wrappper, for a 
    image centered according to the ImageNet mean
    
    This version is for tensorflow 1.2
    """
    dim1,height,width,N = init_img.shape
    if(BGR==False):
        # Bounds from [0,255] - [124,103] if ImageNet 
        bnd_inf = clip_value_min*np.ones((dim1,height,width,N)).flatten() 
        # We need to flatten the array in order to use it in the LBFGS algo
        bnd_sup = clip_value_max*np.ones((dim1,height,width,N)).flatten()
        bnds = np.stack((bnd_inf, bnd_sup),axis=-1)
    else:
        # To bound this variable :  tf.Variable(np.zeros((1, height, width, numberChannels), dtype=np.float32)) by pair (min,max)
        bnd_inf_B = clip_value_min[0]*np.ones((dim1,height,width))
        bnd_inf_G = clip_value_min[1]*np.ones((dim1,height,width))
        bnd_inf_R = clip_value_min[2]*np.ones((dim1,height,width))
        bnd_inf = np.stack((bnd_inf_B, bnd_inf_G,bnd_inf_R),axis=-1)
        bnd_inf = bnd_inf.flatten()
        bnd_sup_B = clip_value_max[0]*np.ones((dim1,height,width))
        bnd_sup_G = clip_value_max[1]*np.ones((dim1,height,width))
        bnd_sup_R = clip_value_max[2]*np.ones((dim1,height,width))
        bnd_sup = np.stack((bnd_sup_B, bnd_sup_G,bnd_sup_R),axis=-1)
        bnd_sup = bnd_sup.flatten()
        bnds = np.stack((bnd_inf, bnd_sup),axis=-1)
    assert len(bnd_sup) == len(init_img.flatten()) # Check if the dimension is right
    assert len(bnd_inf) == len(init_img.flatten()) 

    # Test
    x0 = np.asarray(init_img).ravel()
    n, = x0.shape
    if len(bnds) != n:
        print("Erreur a venir car n",n,"len(bnds)",len(bnds))
    return(bnds)

def get_lbfgs_bnds(init_img,clip_value_min,clip_value_max,BGR=False):
    """
    This function create the bounds for the LBFGS scipy wrappper, for a 
    image centered according to the ImageNet mean
    
    init_img is only used for the dimension of the image
    clip_value_min,clip_value_max compute on the style image before normally
    
    This function return bounds for tensorflow >= 1.3 to be used with the scipy 
    optimizer
    
    """
    dim1,height,width,N = init_img.shape
    if(BGR==False):
        # Bounds from [0,255] - [124,103] if ImageNet 
        bnd_inf = clip_value_min*np.ones((dim1,height,width,N))
        # We need to flatten the array in order to use it in the LBFGS algo
        bnd_sup = clip_value_max*np.ones((dim1,height,width,N))
        bnds = (bnd_inf, bnd_sup)
    else:
        # To bound this variable :  tf.Variable(np.zeros((1, height, width, numberChannels), dtype=np.float32)) by pair (min,max)
        bnd_inf_B = clip_value_min[0]*np.ones((dim1,height,width))
        bnd_inf_G = clip_value_min[1]*np.ones((dim1,height,width))
        bnd_inf_R = clip_value_min[2]*np.ones((dim1,height,width))
        bnd_inf = np.stack((bnd_inf_B, bnd_inf_G,bnd_inf_R),axis=-1)
        #bnd_inf = np.zeros_like(init_img) 
        #bnd_inf[:,:,:,0] = bnd_inf_B
        #bnd_inf[:,:,:,1] = bnd_inf_G
        #bnd_inf[:,:,:,2] = bnd_inf_R
        bnd_sup_B = clip_value_max[0]*np.ones((dim1,height,width))
        bnd_sup_G = clip_value_max[1]*np.ones((dim1,height,width))
        bnd_sup_R = clip_value_max[2]*np.ones((dim1,height,width))
        bnd_sup = np.stack((bnd_sup_B, bnd_sup_G,bnd_sup_R),axis=-1)
        #bnd_sup = np.zeros_like(init_img) 
        #bnd_sup[:,:,:,0] = bnd_sup_B
        #bnd_sup[:,:,:,1] = bnd_sup_G
        #bnd_sup[:,:,:,2] = bnd_sup_R
        #print(bnd_sup.shape)
        bnds = (bnd_inf, bnd_sup)
    return(bnds)

def get_Gram_matrix_wrap(args,vgg_layers,image_style,pooling_type='avg',padding='SAME'):
    _,image_h_art, image_w_art, _ = image_style.shape
    vgg_name = args.vgg_name
    stringAdd = ''
    if(vgg_name=='normalizedvgg.mat'):
        stringAdd = '_n'
    elif(vgg_name=='imagenet-vgg-verydeep-19.mat'):
        stringAdd = '_v' # Regular one
    elif(vgg_name=='random_net.mat'):
        stringAdd = '_r' # random
    data_style_path = args.data_folder + "gram_"+args.style_img_name+"_"+str(image_h_art)+"_"+str(image_w_art)+"_"+str(pooling_type)+"_"+str(padding)+stringAdd
    if args.GramLightComput:
        data_style_path += '_lightVersion'
    data_style_path += ".pkl"
    if(vgg_name=='random_net.mat'):
        try:
            os.remove(data_style_path)
        except:
            pass
    try:
        if(args.verbose): print("Load Data ",data_style_path)
        dict_gram = pickle.load(open(data_style_path, 'rb'))
    except(FileNotFoundError):
        if(args.verbose): print("The Gram Matrices doesn't exist, we will generate them.")
        dict_gram = get_Gram_matrix(vgg_layers,image_style,pooling_type,padding,args)
        with open(data_style_path, 'wb') as output_gram_pkl:
            pickle.dump(dict_gram,output_gram_pkl)
        if(args.verbose): print("Pickle dumped")
    return(dict_gram)

def get_features_repr_wrap(args,vgg_layers,image_content,pooling_type='avg',padding='SAME'):
    _,image_h, image_w, number_of_channels = image_content.shape 
    data_content_path = args.data_folder +args.content_img_name+"_"+str(image_h)+"_"+str(image_w)+"_"+str(pooling_type)+"_"+str(padding)+".pkl"
    try:
        dict_features_repr = pickle.load(open(data_content_path, 'rb'))
    except(FileNotFoundError):
        if(args.verbose): print("The dictionnary of features representation of content image doesn't exist, we will generate it.")
        dict_features_repr = get_features_repr(vgg_layers,image_content,pooling_type,padding)
        with open(data_content_path, 'wb') as output_content_pkl:
            pickle.dump(dict_features_repr,output_content_pkl)
        if(args.verbose): print("Pickle dumped")
        
    return(dict_features_repr)

def plot_image_with_postprocess(args,image,name="",fig=None):
    """
    Plot the image using matplotlib
    """
    if(fig is None):
        fig = plt.figure()
    plt.imshow(postprocess(image))
    plt.title(name)
    if(args.verbose): print("Plot",name)
    fig.canvas.flush_events()
    time.sleep(10**(-6))
    return(fig)

def get_init_img_wrap(args,output_image_path,image_content):
    """
    Function that regroup different way to create differente value for 
    initial condition 
    """
    if(not(args.start_from_noise)):
        try:
            init_img = preprocess(scipy.misc.imread(output_image_path).astype('float32'))
            if(args.verbose):  print("Use the former image")
        except(FileNotFoundError):
            if(args.verbose): print("Former image not found, use of white noise mixed with the content image as initialization image")
            # White noise that we use at the beginning of the optimization
            init_img = get_init_noise_img(image_content,args.init_noise_ratio,args.init_range)
    elif(args.init =='smooth_grad'):
        if(args.verbose): print("Noisy image generation with a smooth gradient")
        print("Warning this don t take into account init_noise_ratio")
        init_img = get_init_noise_img_smooth_grad(image_content) # TODO add a ratio for this kind of initialization also
    elif(args.init=='Gaussian'):
        if(args.verbose): print("Noisy image generation with a Gaussian white noise")
        init_img = get_init_noise_img_gaussian(image_content,args.init_noise_ratio)
    elif(args.init=='Uniform'):
        if(args.verbose): print("Noisy image generation init_noise_ratio = ",args.init_noise_ratio)
        init_img = get_init_noise_img(image_content,args.init_noise_ratio,args.init_range)
    elif(args.init=='Cst'):
        if(args.verbose): print("Constante image")
        _,image_h, image_w, number_of_channels = image_content.shape 
        noise_img = (127.5*np.ones((image_h, image_w, number_of_channels))).astype('float32')
        init_img = preprocess(noise_img)
    if(args.plot):
        plot_image_with_postprocess(args,init_img.copy(),"Initial Image")
    return(init_img)

def get_upScaleOf(namefile,newscale):
    try:
        init_img = scipy.misc.imread(namefile)
        if not(newscale is None):
            init_img = cv2.resize(init_img,(newscale[1],newscale[0]),interpolation=cv2.INTER_CUBIC)
            # Warning ! cv2.resize need w,h whereas in scale it is provide h*w
        upsampled_img = preprocess(init_img.astype('float32'))
    except(FileNotFoundError):
        print('FileNotFoundError')
        raise(FileNotFoundError)
    return(upsampled_img)
    

def load_img(args,img_name,scale=None):
    """
    This function load the image and convert it to a numpy array and do 
    the preprocessing
    """
    image_path = args.img_folder + img_name +args.img_ext
    new_img_ext = args.img_ext
    try:
        img = scipy.misc.imread(image_path)  # Float between 0 and 255
    except IOError:
        if(args.verbose): print("Exception when we try to open the image, try with a different extension format",str(args.img_ext))
        if(args.img_ext==".jpg"):
            new_img_ext = ".png"
        elif(args.img_ext==".png"):
            new_img_ext = ".jpg"
        try:
            image_path = args.img_folder + img_name +new_img_ext # Try the new path
            img = scipy.misc.imread(image_path,mode='RGB')
            if(args.verbose): print("The image have been sucessfully loaded with a different extension")
        except IOError:
            try:
                image_path = args.img_folder + img_name # Try the new path
                img = scipy.misc.imread(image_path,mode='RGB')
                if(args.verbose): print("The image have been sucessfully loaded without extension")
            except IOError:
                if(args.verbose): print("Exception when we try to open the image, we already test the 2 differents extension and without it.")
                raise
    if(len(img.shape)==2):
        if(args.verbose): print("Convert Grey Scale to RGB")
        img = gray2rgb(img) # Convertion greyscale to RGB
    if not(scale is None):
        img = cv2.resize(img, (scale[1],scale[0]),interpolation=cv2.INTER_AREA) 
        # Warning ! cv2.resize need w,h whereas in scale it is provide h*w
    img = preprocess(img.astype('float32'))
    return(img)

def get_losses(args,sess, net, dict_features_repr,M_dict,image_style,dict_gram,pooling_type,padding,
        former_scale=None,i_lowres=None):
    """ Compute the total loss map of the sub loss """
    #Get the layer used for style and other
    content_layers = []
    if(args.config_layers=='PoolConfig'):
        content_layers = [('conv4_2',1)]
        style_layers = [('conv1_1',1),('pool1',1),('pool2',1),('pool3',1),('pool4',1)]
    elif(args.config_layers=='FirstConvs'):
        content_layers = [('conv4_2',1)]
        style_layers = [('conv1_1',1),('conv2_1',1),('conv3_1',1)]
    elif(args.config_layers=='GatysConfig'):
        content_layers = [('conv4_2',1)]
        style_layers = [('relu1_1',1),('pool1',1),('pool2',1),('pool3',1),('pool4',1)]
    elif(args.config_layers=='DCor'):
        # Pseudo Deep Correlation configuration : in the paper there are not 
        # doing autocorr this way and theay are not doing only that (diversity term)
        gram_style_layers = [('pool1',0.25),('pool2',0.25),('pool3',0.25),('pool4',0.25)]
        style_loss =  sum_style_losses(sess, net, dict_gram,M_dict,gram_style_layers)
        list_loss =  [style_loss]
        list_loss_name =  ['style_loss']
        gamma_autocorr = 10**(-4)
        deepcoor_layer = [('pool2',1)]
        autocorr_loss = loss_autocorr(sess,net,image_style,M_dict,deepcoor_layer,gamma_autocorr)
        list_loss +=  [autocorr_loss]
        list_loss_name +=  ['autocorr_loss']
        if(args.type_of_loss=='add'):
            loss_total = tf.reduce_sum(list_loss)
        list_loss +=  [loss_total]
        list_loss_name +=  ['loss_total']
        return(loss_total,list_loss,list_loss_name)
    elif(args.config_layers=='Custom'):
        content_layers =  list(zip(args.content_layers, args.content_layer_weights))
        style_layers = list(zip(args.style_layers,args.style_layer_weights))
    if(args.verbose): print('content_layers',content_layers)
    if(args.verbose): print('style_layers',style_layers)
    
    loss_total = tf.constant(0.)
    list_loss =  []
    list_loss_name =  []
    assert len(args.loss)
    if('content' in args.loss) or ('GatysStyleTransfer'  in args.loss) or ('full' in args.loss):
        content_loss = args.content_strengh * sum_content_losses(sess, net, dict_features_repr,M_dict,content_layers) # alpha/Beta ratio 
        list_loss +=  [content_loss]
        list_loss_name +=  ['content_loss']
    if('texture'  in args.loss) or('Gatys' in args.loss) or('GatysStyleTransfer' in args.loss)  or ('full' in args.loss):
        style_loss =  sum_style_losses(sess, net, dict_gram,M_dict,style_layers)
        list_loss +=  [style_loss]
        list_loss_name +=  ['style_loss']
    if('texMask'  in args.loss):
        mask_dict = pickle.load(open('mask_dict.pkl', 'rb'))
        texture_mask_loss = texture_loss_wt_mask(sess, net, dict_gram,M_dict,mask_dict,style_layers)
        list_loss +=  [texture_mask_loss]
        list_loss_name +=  ['texture_mask_loss']
    if('4moments' in args.loss):
        style_stats_loss = sum_style_stats_loss(sess,net,image_style,M_dict,style_layers)
        list_loss +=  [style_stats_loss]
        list_loss_name +=  ['style_stats_loss']
    if('InterScale'  in args.loss) or ('full' in args.loss):
         inter_scale_loss = loss_crosscor_inter_scale(sess,net,image_style,M_dict,style_layers,sampling=args.sampling,pooling_type=pooling_type)
         list_loss +=  [inter_scale_loss]
         list_loss_name +=  ['inter_scale_loss']
    if('nmoments'  in args.loss) or ('full' in args.loss):
        loss_n_moments_val = loss_n_stats(sess,net,image_style,M_dict,args.n,style_layers,TypeOfComputation='moments')
        list_loss +=  [loss_n_moments_val]
        list_loss_name +=  ['loss_n_moments_val with (n = '+str(args.n)+')']    
    if('nmoments_reduce'  in args.loss):
        loss_n_moments_val = loss_n_stats(sess,net,image_style,M_dict,args.n,style_layers,TypeOfComputation='nmoments_reduce')
        list_loss +=  [loss_n_moments_val]
        list_loss_name +=  ['loss_n_moments_reduce_val with (n = '+str(args.n)+')']     
    if('Lp'  in args.loss) or ('full' in args.loss):
        loss_L_p_val =  loss_n_stats(sess,net,image_style,M_dict,args.p,style_layers,TypeOfComputation='Lp')
        list_loss +=  [loss_L_p_val]
        list_loss_name +=  ['loss_L_p_val with (p = '+str(args.p)+')']  
    if('TV'  in args.loss) or ('full' in args.loss):
        tv_loss =  sum_total_variation_losses(sess, net,args.alpha_TV)
        list_loss +=  [tv_loss]
        list_loss_name +=  ['tv_loss']
    if('TVronde'  in args.loss) or ('full' in args.loss):
        alpha_TVronde = 10
        tv_loss =  sum_TV_ronde_losses(sess, net,alpha_TVronde)
        list_loss +=  [tv_loss]
        list_loss_name +=  ['tv_ronde_loss']
    if('TV1'  in args.loss) :
        tv_norm1_loss =  sum_total_variation_losses_norm1(sess, net,args.alpha_TV)
        list_loss +=  [tv_norm1_loss]
        list_loss_name +=  ['tv_norm1_loss']
    if('bizarre'  in args.loss) or ('full' in args.loss):
         autocorrbizarre_loss = loss_autocorrbizarre(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [autocorrbizarre_loss]
         list_loss_name +=  ['autocorrbizarre_loss']   
    if('autocorr'  in args.loss) or ('full' in args.loss): 
         autocorr_loss = loss_autocorr(sess,net,image_style,M_dict,style_layers,args.gamma_autocorr)
         list_loss +=  [autocorr_loss]
         list_loss_name +=  ['autocorr_loss']
    if('autocorrLog'  in args.loss): 
         autocorr_lossLog = loss_autocorrLog(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [autocorr_lossLog]
         list_loss_name +=  ['autocorr_lossLog']
    if('autocorr_rfft'  in args.loss) or ('full' in args.loss):
         autocorr_rfft_loss =  loss_autocorr_rfft(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [autocorr_rfft_loss]
         list_loss_name +=  ['autocorr_rfft_loss']
    if('fft3D'  in args.loss) or ('full' in args.loss): 
         fft3D_loss = loss_fft3D(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [fft3D_loss]
         list_loss_name +=  ['fft3D_loss']  
    if('fftVect'  in args.loss) or ('full' in args.loss):
         fftVect_loss = loss_fft_vect(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [fftVect_loss]
         list_loss_name +=  ['fftVect_loss'] 
    if('spectrum'  in args.loss) or ('full' in args.loss):
         spectrum_loss = loss_spectrum(sess,net,image_style,M_dict,args.beta_spectrum,eps=args.eps)
         list_loss +=  [spectrum_loss]
         list_loss_name +=  ['spectrum_loss']  
    if('spectrumTFabs'  in args.loss) or ('full' in args.loss):
         spectrumTFabs_loss = loss_spectrumTFabs(sess,net,image_style,M_dict,args.beta_spectrum,eps=args.eps)
         list_loss +=  [spectrumTFabs_loss]
         list_loss_name +=  ['spectrumTFabs_loss']  
    if('spectrumTest'  in args.loss) or ('full' in args.loss):
         spectrumTest_loss,_ = loss_spectrumTFabs_WithGrad(sess,net,image_style,M_dict,args.beta_spectrum,eps=args.eps)
         list_loss +=  [spectrumTest_loss]
         list_loss_name +=  ['spectrumTest_loss']  
    if('variance'  in args.loss) or ('full' in args.loss):
         variance_loss = loss_variance(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [variance_loss]
         list_loss_name +=  ['variance_loss']  
    if('SpectrumOnFeatures'  in args.loss) or ('full' in args.loss):
         SpectrumOnFeatures_loss = loss_SpectrumOnFeatures(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [SpectrumOnFeatures_loss]
         list_loss_name +=  ['SpectrumOnFeatures_loss'] 
    if('phaseAlea' in args.loss) or ('full' in args.loss):
         image_style_Phase = compute_ImagePhaseAlea(sess,net,image_style,M_dict,style_layers)
         phaseAlea_loss = loss_PhaseAleatoire(sess,net,image_style,image_style_Phase,M_dict,style_layers,args.alpha_phaseAlea,args.gamma_phaseAlea)
         list_loss +=  [phaseAlea_loss]
         list_loss_name +=  ['phaseAlea_loss']  
    if('entropy' in args.loss) or ('full' in args.loss): 
         loss_entropy_value = loss_entropy(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [loss_entropy_value]
         list_loss_name +=  ['loss_entropy_value'] 
    if('phaseAleaSimple' in args.loss):
         image_style_Phase = compute_ImagePhaseAlea(sess,net,image_style,M_dict,style_layers)
         phaseAlea_loss = loss_PhaseAleatoireSimple(sess,net,image_style,image_style_Phase,M_dict,style_layers)
         list_loss +=  [phaseAlea_loss]
         list_loss_name +=  ['phaseAlea_loss_simple']  
    if('phaseAleaList' in args.loss) or ('full' in args.loss):
         image_style_Phase = compute_ImagePhaseAlea_list(sess,net,image_style,M_dict,style_layers)
         phaseAleaList_loss = loss_PhaseAleatoirelist(sess,net,image_style,image_style_Phase,M_dict,style_layers)
         list_loss +=  [phaseAleaList_loss]
         list_loss_name +=  ['phaseAleaList_loss']           
    if('intercorr' in args.loss) or ('full' in args.loss):
         print("Risk to do a Ressource Exhausted error :) ")
         intercorr_loss = loss_intercorr(sess,net,image_style,M_dict,style_layers)
         list_loss +=  [intercorr_loss]
         list_loss_name +=  ['intercorr_loss']  
    if('current' in args.loss) or ('full' in args.loss): 
         PhaseImpose_loss = loss_PhaseImpose(sess,net,image_style,M_dict,style_layers)   
         list_loss +=  [PhaseImpose_loss]
         list_loss_name +=  ['PhaseImpose_loss']    
    if('HF' in args.loss) or ('full' in args.loss):
         HF_loss = loss__HF_filter(sess, net, image_style,M_dict)   
         list_loss +=  [HF_loss]
         list_loss_name +=  ['HF_loss']
    if('HFmany' in args.loss):
         HFmany_loss = loss__HF_many_filters(sess, net, image_style,M_dict)   
         list_loss +=  [HFmany_loss]
         list_loss_name +=  ['HFmany_loss'] 
         
    # If we have a strong constraint on a low resolution version of the image
    if args.MS_Strat == 'Constr' and not(i_lowres is None): 
        LowResConstr_loss = loss_LowResConstr(sess,net,former_scale,i_lowres,weightMultiplicatif=args.WLowResConstr)   
        list_loss +=  [LowResConstr_loss]
        list_loss_name +=  ['LowResConstr_loss']
         
    if(args.type_of_loss=='add'):
        loss_total = tf.reduce_sum(list_loss)
    elif(args.type_of_loss=='max'):
        loss_total = tf.reduce_max(list_loss)
    elif(args.type_of_loss=='mul'):
        # If one of the sub loss is zero the total loss is zero !
        if(args.verbose): print("Mul for the total loss : If one of the sub loss is zero the total loss is zero.")
        loss_total = tf.constant(1.)
        for loss in list_loss:
            loss_total *= (loss*10**(-9)) 
    elif(args.type_of_loss=='Keeney'):
        if(args.verbose): print("Keeney for the total loss : they are a lot of different weight everywhere.")
        loss_total = tf.constant(1.*10**9)
        for loss in list_loss:
            loss_total *= (loss*10**(-9) + 1.) 
        #Seem to optimize quickly but is stuck
    else:
        if(args.verbose): print("The loss aggregation function is not known")
    list_loss +=  [loss_total]
    list_loss_name +=  ['loss_total']
    return(loss_total,list_loss,list_loss_name)
    
def get_scales(image_h,image_w,MS_minscale,K):
    """
    This function produce the list of scale for the image synthesis
    If K!=-1 we can work with no squared image
    """
    list_scale = []
    if K==-1:
        if image_h < image_w:
            dim_image = image_h
            MS_minscale_h = MS_minscale
            MS_minscale_w = int(np.floor(MS_minscale*image_w/image_h))
        else:
            dim_image = image_w
            MS_minscale_h = int(np.floor(MS_minscale*image_h/image_w))
            MS_minscale_w = MS_minscale
        while (MS_minscale <= dim_image):
            list_scale += [[MS_minscale_h,MS_minscale_w]]
            MS_minscale_h *= 2
            MS_minscale_w *= 2
            if MS_minscale_h==image_h:
                MS_minscale_w = image_w
            if MS_minscale_w==image_w:
                MS_minscale_h = image_h
        if not(list_scale[-1][0]==image_h) or not(list_scale[-1][1]==image_w):
            list_scale += [[image_h,image_w]]
    else:
        for i in range(-K,1):
            list_scale += [[int(np.floor(image_h*(2**i))),int(np.floor(image_w*(2**i)))]]
    return(list_scale)
    
    
def style_transfer(args):
    """
    This function is the main core of the program it need args in order to
    set up all the things and run an optimization in order to produce an 
    image 
    """
    #tf.enable_eager_execution()
    if args.verbose:
        tinit = time.time()
        print("verbosity turned on")
        print(args)
    
    if args.max_iter < args.print_iter:
        args.print_iter = args.max_iter
    
    output_image_path_first = args.img_output_folder + args.output_img_name + args.img_ext
    if(args.verbose and args.img_ext=='.jpg'): print("Be careful you are saving the image in JPEG !")
    image_content_first = load_img(args,args.content_img_name)
    image_style_first = load_img(args,args.style_img_name)
    _,image_h_first, image_w_first, number_of_channels = image_content_first.shape 
        
    if args.MS_Strat in ['Init','Constr']:
        if args.verbose: print("I would like to warm you up that the resize from TF I use are not really good :( Sorry")
        reDo = True # This is a bad way to do it but the only solution !
        if not(image_h_first%args.MS_minscale==0) or not(image_w_first%args.MS_minscale==0):
            if args.verbose: print('It seems that the scale is not divised by the  args.MS_minscale we will deal with it for the last scale')
        if image_h_first < args.MS_minscale:
            print('image_h < args.MS_minscale No multiscale strategy')
        list_scale = get_scales(image_h_first,image_w_first,args.MS_minscale,args.K)
        tmp_output_image_path = args.img_output_folder + 'TextureForMSStrat' + args.img_ext
    else:
        list_scale = [[image_h_first,image_w_first]]
    former_scale = None
    i_lowres = None
        
    # Loop on the scale
    for i_scale,scales in enumerate(list_scale):
        scale_h,scale_w = scales
        if args.verbose and not(args.MS_Strat==''):
            print('== Scale number ',str(1+i_scale),'on ',len(list_scale),' equal to ',scale_h,' * ',scale_w,'scale',args.MS_Strat,' ==')
        if scale_h == image_h_first:
            image_h = image_h_first
            image_w = image_w_first
            image_content = image_content_first
            image_style = image_style_first
            output_image_path = output_image_path_first
        else:
            image_h = scale_h
            image_w = scale_w
            image_content = load_img(args,args.content_img_name,scale=[scale_h,scale_w])
            image_style = load_img(args,args.style_img_name,scale=[scale_h,scale_w])
            output_image_path = tmp_output_image_path
            if args.saveMS or args.savedIntermediateIm: 
                image_style_pp = postprocess(image_style.copy())
                tmp_path_wt_ext = args.img_output_folder + args.style_img_name + 'Ref_' + str(list_scale[i_scale][0])
                tmp_path = tmp_path_wt_ext + args.img_ext  
                scipy.misc.toimage(image_style_pp).save(tmp_path)
         
        # Definition of the callback for the optimization
        global _iter
        _iter = 0
        def print_loss_tab_callback(list_loss,list_loss_name):
            global _iter
            strToPrint = ''
            for loss,loss_name in zip(list_loss,list_loss_name):
                loss_tmp = sess.run(loss)
                strToPrint +=  loss_name + ' = {:.2e}, '.format(loss_tmp)
            print(strToPrint)
            _iter += 1
            
        if(args.clipping_type=='ImageNet'):
            BGR=False
            clip_value_min,clip_value_max = get_clip_values(None,BGR)
        elif(args.clipping_type=='ImageStyle'):
            BGR=False
            clip_value_min,clip_value_max = get_clip_values(image_style,BGR)
        elif(args.clipping_type=='ImageStyleBGR'):
            BGR = True
            clip_value_min,clip_value_max = get_clip_values(image_style,BGR)

        if(args.plot):
            plt.ion()
            plot_image_with_postprocess(args,image_content.copy(),"Content Image")
            plot_image_with_postprocess(args,image_style.copy(),"Style Image")
            fig = None # initialization for later
            
        # TODO add something that reshape the image 
        t1 = time.time()
        pooling_type = args.pooling_type
        padding = args.padding
        vgg_layers = get_vgg_layers(args.vgg_name)
        
        # Precomputation Phase :
        if args.MS_Strat in ['Init','Constr'] or args.GramLightComput:
            if args.verbose: print("In those cases we will not use the precomputed gram matrix")
            dict_gram = get_Gram_matrix(vgg_layers,image_style,pooling_type,padding,args)
        else:
            dict_gram = get_Gram_matrix_wrap(args,vgg_layers,image_style,pooling_type,padding)
        if ('full' in args.loss) or('Gatys' in args.loss) or ('content' in args.loss):
            if not(padding=='Davy'):
                dict_features_repr = get_features_repr_wrap(args,vgg_layers,image_content,pooling_type,padding)
            else:
                raise(utils.MyError("It is not allowed to use Davy padding for doing Style Transfer, at least you find how to do. Good luck :)"))
        else: 
            dict_features_repr = None
            
        if padding=='Davy':
            # In this case the content matrice is just use for the output size
            image_content = np.zeros((1,2*image_h, 2*image_w, number_of_channels)).astype('float32')
            M_dict = get_M_dict_Davy(2*image_h,2*image_w)
        elif padding=='VALID':
            M_dict = get_M_dict_Davy(image_h,image_w)
        else:       
            M_dict = get_M_dict(image_h,image_w)
            
        net = net_preloaded(vgg_layers,image_content,pooling_type,padding) # The output image as the same size as the content one
        
        t2 = time.time()
        if(args.verbose): print("net loaded and gram computation after ",t2-t1," s")

        try:
            config = tf.ConfigProto()
            if(args.gpu_frac <= 0.):
                config.gpu_options.allow_growth = True
                if args.verbose: print("Memory Growth")
            elif(args.gpu_frac <= 1.):
                config.gpu_options.per_process_gpu_memory_fraction = args.gpu_frac
                if args.verbose: print("Becareful args.gpu_frac = ",args.gpu_frac,"It may cause problem if the value is superior to the available memory place.")
            sess = tf.Session(config=config)

            
            if args.MS_Strat=='Constr' and not(i_scale==0):
                if args.saveMS: saveImExt(tmp_output_image_path,args.style_img_name,str(list_scale[i_scale-1]),args.img_output_folder)
                init_img = get_upScaleOf(tmp_output_image_path,[scale_h,scale_w])
                former_scale = list_scale[i_scale-1]
                i_lowres = get_upScaleOf(tmp_output_image_path,None)
            elif args.MS_Strat=='Init' and not(i_scale==0):
                if args.saveMS: saveImExt(tmp_output_image_path,args.style_img_name,str(list_scale[i_scale-1]),args.img_output_folder)
                init_img = get_upScaleOf(tmp_output_image_path,[scale_h,scale_w])
            else:
                init_img = get_init_img_wrap(args,output_image_path,image_content)
            
            loss_total,list_loss,list_loss_name = get_losses(args,sess, net, dict_features_repr,\
                M_dict,image_style,dict_gram,pooling_type,padding,former_scale=former_scale,i_lowres=i_lowres)
                
            # Preparation of the assignation operation
            placeholder = tf.placeholder(tf.float32, shape=init_img.shape)
            placeholder_clip = tf.placeholder(tf.float32, shape=init_img.shape)
            print(init_img.shape)
            assign_op = net['input'].assign(placeholder)
            clip_op = tf.clip_by_value(placeholder_clip,clip_value_min=np.mean(clip_value_min),clip_value_max=np.mean(clip_value_max),name="Clip") # The np.mean is a necessity in the case whe got the BGR values TODO : need to change all that
            
            if(args.verbose): print("init loss total")

            if(args.optimizer=='adam'): # Gradient Descent with ADAM algo
                optimizer = tf.train.AdamOptimizer(args.learning_rate)
            elif(args.optimizer=='GD'): # Gradient Descente 
                if((args.learning_rate > 1) and (args.verbose)): print("We recommande you to use a smaller value of learning rate when using the GD algo")
                optimizer = tf.train.GradientDescentOptimizer(args.learning_rate)
                
            if((args.optimizer=='GD') or (args.optimizer=='adam')):
                if args.savedIntermediateIm:
                    raise(NotImplemented) # TODO
                    
                train = optimizer.minimize(loss_total)

                sess.run(tf.global_variables_initializer())
                sess.run(assign_op, {placeholder: init_img})
                            
                sess.graph.finalize() # To test if the graph is correct
                if(args.verbose): print("sess.graph.finalize()") 

                t3 = time.time()
                if(args.verbose): print("sess Adam initialized after ",t3-t2," s")
                # turn on interactive mode
                if(args.verbose): print("loss before optimization")
                if(args.verbose): print_loss_tab(sess,list_loss,list_loss_name)
                for i in range(args.max_iter):
                    if(i%args.print_iter==0):
                        if(args.tf_profiler):
                            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
                            run_metadata = tf.RunMetadata()
                            sess.run(train,options=run_options, run_metadata=run_metadata)
                            # Create the Timeline object, and write it to a json
                            tl = timeline.Timeline(run_metadata.step_stats)
                            ctf = tl.generate_chrome_trace_format()
                            if(args.verbose): print("Time Line generated")
                            nameFile = 'timeline'+str(i)+'.json'
                            with open(nameFile, 'w') as f:
                                if(args.verbose): print("Save Json tracking")
                                f.write(ctf)
                                # Read with chrome://tracing
                        else:
                            t3 =  time.time()
                            sess.run(train)
                            t4 = time.time()
                            result_img = sess.run(net['input'])
                            if(args.clip_var==1): # Clipping the variable
                                cliptensor = sess.run(clip_op,{placeholder_clip: result_img})
                                sess.run(assign_op, {placeholder: cliptensor})
                            if(args.verbose): print("Iteration ",i, "after ",t4-t3," s")
                            if(args.verbose): print_loss_tab(sess,list_loss,list_loss_name)
                            if(args.plot): fig = plot_image_with_postprocess(args,result_img,"Intermediate Image",fig)
                            result_img_postproc = postprocess(result_img)
                            scipy.misc.toimage(result_img_postproc).save(output_image_path)
                    else:
                        # Just training
                        sess.run(train)
                        if(args.clip_var==1): # Clipping the variable
                            result_img = sess.run(net['input'])
                            cliptensor = sess.run(clip_op,{placeholder_clip: result_img})
                            sess.run(assign_op, {placeholder: cliptensor}) 
            elif(args.optimizer=='lbfgs'):
                # TODO : be able to detect of print_iter > max_iter and deal with it
                nb_iter = args.max_iter  // args.print_iter
                max_iterations_local = args.max_iter // nb_iter
                if(args.verbose): print("Start LBFGS optim with a print each ",max_iterations_local," iterations")
                print_iterations = args.iprint if args.verbose else 0 # Number of iterations between optimizer print statements.
                
                #if args.savedIntermediateIm:
                    #callback = print_loss_and_savedIm_callback
                #else:
                    ##callback = partial(print_loss_tab_callback,list_loss_name=list_loss_name)
                    #callback = print_loss_tab_callback(list_loss,list_loss_name=list_loss_name)
                
                optimizer_kwargs = {'maxiter': max_iterations_local,'maxcor': args.maxcor, \
                    'disp': print_iterations}
                #,'callback':callback
                # To solve the non retro compatibility of Tensorflow !
                if test_version_sup('1.3'): # TODO change it : this string comparison don t work for version > 1.10
                    bnds = get_lbfgs_bnds(init_img,clip_value_min,clip_value_max,BGR)
                    trainable_variables = tf.trainable_variables()[0]
                    var_to_bounds = {trainable_variables: bnds}
                    optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss_total,var_to_bounds=var_to_bounds,
                        method='L-BFGS-B',options=optimizer_kwargs)   
                else:
                    bnds = get_lbfgs_bnds_tf_1_2(init_img,clip_value_min,clip_value_max,BGR)
                    optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss_total,bounds=bnds,
                        method='L-BFGS-B',options=optimizer_kwargs)    
                sess.run(tf.global_variables_initializer())
                sess.run(assign_op, {placeholder: init_img})
                            
                sess.graph.finalize() # To test if the graph is correct
                if(args.verbose): print("sess.graph.finalize()") 
                
                if(args.verbose): print("loss before optimization")
                if(args.verbose): print_loss_tab(sess,list_loss,list_loss_name)
                
                #optimizer.minimize(sess, init_img, loss_callback=callback)
                
                for i in range(nb_iter):
                    t3 =  time.time()
                    optimizer.minimize(sess)
                    t4 = time.time()
                    if(args.verbose): print("Iteration ",i, "after ",t4-t3," s")
                    if(args.verbose): print_loss_tab(sess,list_loss,list_loss_name)
                    result_img = sess.run(net['input'])
                    if(args.plot): fig = plot_image_with_postprocess(args,result_img.copy(),"Intermediate Image",fig)
                    if(padding=='Davy'):
                        result_img_postproc = postprocess(utils.get_center_tensor(result_img))
                    else:
                        result_img_postproc = postprocess(result_img.copy())
                    scipy.misc.imsave(output_image_path,result_img_postproc)
            
            # The last iterations are not made
            # The End : save the resulting image
            result_img = sess.run(net['input'])
            if(args.plot): plot_image_with_postprocess(args,result_img.copy(),"Final Image",fig)
            if(padding=='Davy'):
                result_img_postproc_noCrop = postprocess(result_img.copy())
                temp, ext = output_image_path.split('.')
                output_image_path2 = temp + '_noCrop' +'.'+ ext 
                scipy.misc.toimage(result_img_postproc_noCrop).save(output_image_path2) # Save the complet image
                result_img_postproc = postprocess(utils.get_center_tensor(result_img.copy())) # Save the crop image
            else:
                result_img_postproc = postprocess(result_img.copy())
            scipy.misc.toimage(result_img_postproc).save(output_image_path) 
            if args.HistoMatching:
                # Histogram Matching
                if(args.verbose): print("Histogram Matching before saving")
                result_img_postproc = Misc.histogram_matching(result_img_postproc.copy(), postprocess(image_style.copy()))
                output_image_path_hist = args.img_output_folder + args.output_img_name+'_hist' +args.img_ext
                scipy.misc.toimage(result_img_postproc).save(output_image_path_hist.copy())   
            
        except:
            if(args.verbose): print("Error, in the lbfgs case the image can be strange and incorrect")
            result_img = sess.run(net['input'])
            if(padding=='Davy'):
                result_img_postproc = postprocess(utils.get_center_tensor(result_img))
            else:
                result_img_postproc = postprocess(result_img)
            output_image_path_error = args.img_output_folder + args.output_img_name+'_error' +args.img_ext
            scipy.misc.toimage(result_img_postproc).save(output_image_path_error)
            # In the case of the lbfgs optimizer we only get the init_img if we did not do a check point before
            raise 
        finally:
            sess.close()
            tf.reset_default_graph()
            if(args.verbose): 
                print("Close Sess - End of the global optimization.")
                tend = time.time()
                print("Computation total for ",tend-tinit," s")
        if(args.plot): input("Press enter to end and close all")

def main():
    #global args
    parser = get_parser_args()
    args = parser.parse_args()
    style_transfer(args)

def main_with_option():
    parser = get_parser_args()
    brick = "mesh_texture_surface_2048"
    brick = "mesh_texture_surface_4096"
    brick = "lego_1024"
    #brick = "TexturesCom_BrickSmallNew0099_1_seamless_S_1024"
    #brick = "StarryNight"
    img_folder = "HDImages/"
    #img_folder = "dataImages2/"
    img_output_folder = "dataImages2/"
    image_style_name = brick
    content_img_name  = brick
    max_iter = 2000
    print_iter = 2000
    start_from_noise = 1 # True
    init_noise_ratio = 1.0 # TODO add a gaussian noise on the image instead a uniform one
    content_strengh = 0.001
    optimizer = 'lbfgs'
    learning_rate = 1 # 10 for adam and 10**(-10) for GD
    maxcor = 20
    sampling = 'up'
    MS_Strat = 'Init'
    MS_Strat = ''
    loss = ['texture','spectrumTFabs']
    loss = ['texture']
    saveMS = True
    #MS_Strat = 'Constr'
    #MS_Strat = ''
    eps = 10**(-16)
    # In order to set the parameter before run the script
    parser.set_defaults(style_img_name=image_style_name,max_iter=max_iter,img_folder=img_folder,
        print_iter=print_iter,start_from_noise=start_from_noise,img_output_folder=img_output_folder,
        content_img_name=content_img_name,init_noise_ratio=init_noise_ratio,
        content_strengh=content_strengh,optimizer=optimizer,maxcor=maxcor,
        learning_rate=learning_rate,sampling=sampling,MS_Strat=MS_Strat,loss=loss,saveMS=saveMS,eps=eps)
    args = parser.parse_args()
    style_transfer(args)
    
#def main_with_option2():
    #parser = get_parser_args()
    #image_style_name= "StarryNight_Big"
    #image_style_name= "StarryNight"
    #starry = "StarryNight"
    #marbre = 'GrungeMarbled0021_S'
    #tile =  "TilesOrnate0158_1_S"
    #tile2 = "TilesZellige0099_1_S"
    #peddle = "pebbles"
    #brick = "BrickSmallBrown0293_1_S"
    ##D ="D20_01"
    ##orange = "orange"
    ##bleu = "bleu"
    ##glass = "glass"
    ##damier ='DamierBig_Proces'
    ##camouflage = 'Camouflage0003_S'
    ##brick = 'Brick_512_1'
    ###brick = 'MarbreWhite_1'
    ###brick = 'Camouflage_1'
    ##brick = 'OrnamentsCambodia0055_512'
    #brick = "BrickRound0122_1_seamless_S"
    #brick = "lego_1024maison"
    #brick = "lego_1024Ref_256"
    #output_img_name = 'lego_1024Ref_256_maison_000step'
    #img_folder = "images/"
    #img_output_folder = "images/"
    #image_style_name = brick
    #content_img_name  = brick
    #max_iter = 2000
    #print_iter = 2000
    #start_from_noise = 1 # True
    #init_noise_ratio = 1.0 # TODO add a gaussian noise on the image instead a uniform one
    #content_strengh = 0.001
    #optimizer = 'lbfgs'
    #learning_rate = 1 # 10 for adam and 10**(-10) for GD
    #maxcor = 20
    #sampling = 'up'
    #MS_Strat = 'Init'
    #loss = ['texture','spectrum']
    #saveMS = True
    ##MS_Strat = 'Constr'
    ##MS_Strat = ''
    ## In order to set the parameter before run the script
    #parser.set_defaults(style_img_name=image_style_name,max_iter=max_iter,img_folder=img_folder,
        #print_iter=print_iter,start_from_noise=start_from_noise,img_output_folder=img_output_folder,
        #content_img_name=content_img_name,init_noise_ratio=init_noise_ratio,output_img_name=output_img_name,
        #content_strengh=content_strengh,optimizer=optimizer,maxcor=maxcor,
        #learning_rate=learning_rate,sampling=sampling,MS_Strat=MS_Strat,loss=loss,saveMS=saveMS)
    #args = parser.parse_args()
    #style_transfer(args)

if __name__ == '__main__':
    #main() # Command line : python Style_Transfer.py --content_img_name VG --style_img_name estampe --print_iter 1000 --max_iter 1000 --loss texture content --HistoMatching
    main_with_option()
    # Use CUDA_VISIBLE_DEVICES='' python ... to avoid using CUDA
    # Pour update Tensorflow : python3.6 -m pip install --upgrade tensorflow-gpu
    
    
